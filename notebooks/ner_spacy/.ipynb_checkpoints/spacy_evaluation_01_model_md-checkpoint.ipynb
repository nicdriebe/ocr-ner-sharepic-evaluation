{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eaa098a-e21d-4e0f-907a-9e80f57b62cd",
   "metadata": {},
   "source": [
    "# SpaCy Evaluation - 01 -\n",
    "\n",
    "In diesem notebook wird untersucht wie gut SpaCy die gewünschten Entitäten (EVENT, TOPIC, DATE, TIME, LOC) erkennt.\n",
    "\n",
    "Es wird folgendes Model genutzt: **\"de_core_news_md\"**   \n",
    "realtiv leichtgewichtig ( 42 MB)  \n",
    "optimiert für CPU  \n",
    "F1-Score: 00,84   \n",
    "kann vier Entitäten erkennen:  \n",
    "PER, LOC, ORG, MISC  \n",
    "\n",
    "Es wird nur untersucht wie gut LOC im Datensatz rkannt wird.  \n",
    "\n",
    "\n",
    "Die Performance von SpaCy wird auf dem ground truth untersucht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2431b5-e5c7-4919-919d-a9b4da189f6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65784ce-0029-4197-bd24-0adf58cb9d10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-md==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/de_core_news_md-3.8.0/de_core_news_md-3.8.0-py3-none-any.whl (44.4 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "#download model\n",
    "!python -m spacy download de_core_news_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e20449-fa3c-4731-a8c4-0521c28333cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LOC', 'MISC', 'ORG', 'PER')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Modell laden \n",
    "nlp = spacy.load(\"de_core_news_md\") \n",
    "\n",
    "# Alle Entity-Labels ausgeben\n",
    "print(nlp.get_pipe(\"ner\").labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f69e8-76b3-4bab-8113-09bc9abaaa33",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Test auf einem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036d0c62-37a6-4e08-b376-4f8e097a93ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPRACHCAFE 0 10 ORG\n",
      "WIR FREUEN 69 79 ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "doc = nlp(\"SPRACHCAFE WEIHNACHTSFEIER DIENSTAG 21.12. OLOF-PALME ZENTRUM 19 UHR WIR FREUEN UNS AUF EUCH!\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececf556-0902-4edb-aa8e-fbccf35526c5",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Evaluation auf gesamtem Datensatz \n",
    "\n",
    "Es wird nicht auf Token-Ebene (einzelne Wörter), sondern auf Span-Ebene untersucht, also auf zusammenhängende Entitäten mit ihrer jeweiligen Start und End Position im Text.  \n",
    "Es wird nur LOC betrachtet.\n",
    "\n",
    "Das Ergebnis wird pro Eintrag in einer Json-Datei gespeichert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e271034b-650a-44bd-9d10-87c8c246b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    all_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b93ef07-2aaa-4af3-8887-5aabb584021f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gesamtbewertung ===\n",
      "Precision: 0.36\n",
      "Recall   : 0.20\n",
      "F1-Score : 0.26\n",
      "\n",
      "=== Bewertung pro Label ===\n",
      "LOC        P: 0.36  R: 0.20  F1: 0.26\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "tp, fp, fn = 0, 0, 0\n",
    "label_stats = defaultdict(lambda: [0, 0, 0])  # TP, FP, FN pro Label\n",
    "relevant_labels = {\"LOC\"}\n",
    "all_results = []\n",
    "\n",
    "# Funktion zur erstellung des dictionary\n",
    "def span_to_dict(span, text):\n",
    "    return {\n",
    "        \"text\": text[span[0]:span[1]],\n",
    "        \"start\": span[0],\n",
    "        \"end\": span[1],\n",
    "        \"label\": span[2]\n",
    "    }\n",
    "\n",
    "for eintrag in all_data:\n",
    "    file_name = eintrag.get(\"file_name\", None)\n",
    "    text = eintrag[\"text\"]\n",
    "    gold = eintrag.get(\"entities\", [])\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Gold-Entitäten vorbereiten\n",
    "    gold_ent = {\n",
    "        (e[\"start\"], e[\"end\"], e[\"label\"])\n",
    "        for e in gold\n",
    "        if e[\"label\"] in relevant_labels\n",
    "    }\n",
    "\n",
    "    # Vorhersagen von spaCy\n",
    "    pred_ent = {\n",
    "        (ent.start_char, ent.end_char, ent.label_)\n",
    "        for ent in doc.ents\n",
    "        if ent.label_ in relevant_labels\n",
    "    } \n",
    "    \n",
    "    # Gesamtmetriken\n",
    "    tp += len(gold_ent & pred_ent)\n",
    "    fp += len(pred_ent - gold_ent)\n",
    "    fn += len(gold_ent - pred_ent)\n",
    "\n",
    "    gold_spans = gold_ent\n",
    "    pred_spans = pred_ent\n",
    "\n",
    "\n",
    "    # für Berechnung pro Eintrag\n",
    "    tp_spans = gold_spans & pred_spans\n",
    "    fp_spans = pred_spans - gold_spans\n",
    "    fn_spans = gold_spans - pred_spans\n",
    "    \n",
    "    tp_count = len(tp_spans)\n",
    "    fp_count = len(fp_spans)\n",
    "    fn_count = len(fn_spans)\n",
    "    \n",
    "    # lokale Metriken für dieses Dokument berechnen\n",
    "    precision_local = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "    recall_local = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "    f1_local = 2 * precision_local * recall_local / (precision_local + recall_local) if (precision_local + recall_local) > 0 else 0\n",
    "\n",
    "\n",
    "    # Pro Label\n",
    "    #for label in set([e[\"label\"] for e in gold + doc]):\n",
    "    for label in relevant_labels:\n",
    "        if label not in relevant_labels:\n",
    "            continue \n",
    "    \n",
    "        g = {s for s in gold_spans if s[2] == label}\n",
    "        p = {s for s in pred_spans if s[2] == label}\n",
    "        label_stats[label][0] += len(g & p)      # TP\n",
    "        label_stats[label][1] += len(p - g)      # FP\n",
    "        label_stats[label][2] += len(g - p)      # FN\n",
    "\n",
    "    result = {\n",
    "    \"file_name\": file_name,\n",
    "    \"text\": text,\n",
    "    \"precision\": precision_local,\n",
    "    \"recall\": recall_local,\n",
    "    \"f1\": f1_local,\n",
    "    \"true_positives\": [span_to_dict(s, text) for s in tp_spans],\n",
    "    \"false_positives\": [span_to_dict(s, text) for s in fp_spans],\n",
    "    \"false_negatives\": [span_to_dict(s, text) for s in fn_spans],\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "# Speichern der results / Ergebnis pro Eintrag\n",
    "with open(\"../../data/NER/spacy/results_de_core_news_md.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. Gesamtergebnisse\n",
    "# -------------------------\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n=== Gesamtbewertung ===\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall   : {recall:.2f}\")\n",
    "print(f\"F1-Score : {f1:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Bewertung pro Label\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n=== Bewertung pro Label ===\")\n",
    "for label, (tp_l, fp_l, fn_l) in label_stats.items():\n",
    "    p = tp_l / (tp_l + fp_l) if (tp_l + fp_l) > 0 else 0\n",
    "    r = tp_l / (tp_l + fn_l) if (tp_l + fn_l) > 0 else 0\n",
    "    f = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    print(f\"{label:<10} P: {p:.2f}  R: {r:.2f}  F1: {f:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f176e20-191a-453f-a182-a87b372890c9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4766081-8221-4604-b680-e52c00ca6e9b",
   "metadata": {},
   "source": [
    "#### Untersuchung, ob unterschiedliche Start- und Endpositionen bei predicted und gold standard Ursache für schlechten Wert sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3067ca7-2c09-4bec-9c80-2423eaaa2436",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOC-Ergebnisse ===\n",
      "True Positives (genau): 83\n",
      "Overlap Matches (Text & Label gleich): 86\n",
      "Fuzzy Matches (±2 Zeichen): 87\n",
      "Fuzzy Matches ohne Overlaps: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "with open(\"../../data/NER/spacy/results_de_core_news_md.json\", encoding=\"utf-8\") as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "def span_text(span, text):\n",
    "    return text[span[0]:span[1]]\n",
    "\n",
    "def fuzzy_match(gold_span, pred_span, tolerance=2):\n",
    "    return (\n",
    "        gold_span[2] == pred_span[2] and\n",
    "        abs(gold_span[0] - pred_span[0]) <= tolerance and\n",
    "        abs(gold_span[1] - pred_span[1]) <= tolerance\n",
    "    )\n",
    "\n",
    "# Index predicted nach file_name\n",
    "pred_index = {entry[\"file_name\"]: entry for entry in pred_data}\n",
    "\n",
    "\n",
    "tp = 0\n",
    "overlap_matches = 0  # nur text und label sind gleich\n",
    "fuzzy_matches = 0    # text und label sind gleich und position weicht um maximal 2 Zeichen ab\n",
    "\n",
    "for eintrag in gold_data:\n",
    "    file_name = eintrag.get(\"file_name\")\n",
    "    text = eintrag[\"text\"]\n",
    "    gold = [e for e in eintrag.get(\"entities\", []) if e[\"label\"] == \"LOC\"]\n",
    "\n",
    "    pred_entry = pred_index.get(file_name, {})\n",
    "    # Kombiniere TP und FP, FN interessieren hier nicht\n",
    "    predicted = pred_entry.get(\"true_positives\", []) + pred_entry.get(\"false_positives\", [])\n",
    "    predicted = [e for e in predicted if e[\"label\"] == \"LOC\"]\n",
    "\n",
    "    # Erstelle Sets für TP-Zählung\n",
    "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in gold}\n",
    "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in predicted}\n",
    "\n",
    "    tp += len(gold_spans & pred_spans)\n",
    "\n",
    "    gold_spans_list = list(gold_spans)\n",
    "    pred_spans_list = list(pred_spans)\n",
    "\n",
    "    # Overlap: label und Text gleich\n",
    "    matched_pred_indices = set()\n",
    "    for g in gold_spans:\n",
    "        for i, p in enumerate(pred_spans_list):\n",
    "            if i in matched_pred_indices:\n",
    "                continue\n",
    "            if g[2] == p[2] and span_text(g, text) == span_text(p, text):\n",
    "                overlap_matches += 1\n",
    "                matched_pred_indices.add(i)\n",
    "                break\n",
    "\n",
    "    # Fuzzy Match: ±2 Zeichen Toleranz bei Start/Ende\n",
    "    matched_pred_indices_fuzzy = set()\n",
    "    for g in gold_spans_list:\n",
    "        for i, p in enumerate(pred_spans_list):\n",
    "            if i in matched_pred_indices_fuzzy:\n",
    "                continue\n",
    "            if fuzzy_match(g, p):\n",
    "                fuzzy_matches += 1\n",
    "                matched_pred_indices_fuzzy.add(i)\n",
    "                break\n",
    "\n",
    "print(f\"\\n=== LOC-Ergebnisse ===\")\n",
    "print(f\"True Positives (genau): {tp}\")\n",
    "print(f\"Overlap Matches (Text & Label gleich): {overlap_matches}\")\n",
    "print(f\"Fuzzy Matches (±2 Zeichen): {fuzzy_matches}\")\n",
    "print(f\"Fuzzy Matches ohne Overlaps: {fuzzy_matches - overlap_matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb124c7-799b-4536-8e9a-327367bfedca",
   "metadata": {},
   "source": [
    "##### ---> es wurden 83 Matches gefunden, die genau dieselbe Start und Endposition haben. 4 Matches weichen in Start- und Endposition um maximal zwei Zeichen ab.  \n",
    "##### ---> Die schlechte Vorhersage von LOC liegt nicht primär in den unterschiedlichen Positionen von Goldstandard und prediction begründet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383c520-57b9-4054-aec7-427c6fb803f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "spacy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
