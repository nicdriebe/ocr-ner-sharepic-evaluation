{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae027cd-6a3e-48d5-a4d8-666b73ee0f45",
   "metadata": {},
   "source": [
    "# Gemma 2 Evaluation - 04 -\n",
    "\n",
    "In diesem notebook wird untersucht wie gut Gemma 2 die gew√ºnschten Entit√§ten (EVENT, TOPIC, DATE, TIME, LOC) erkennt.\n",
    "\n",
    "Es wird folgendes Model genutzt: **\"gemma-2-2b-it\"**   \n",
    "kleinstes Modell der Gemma2 Modelle  \n",
    "(2B model was trained with 2 trillion tokens)\n",
    "\n",
    "Die Performance von Gemma 2 wird auf dem Ground Truth untersucht.\n",
    "\n",
    "**erste Quantitative Analyse - Berechnung von Precision, Recall und F1-Score - nur als erster Vergleich zw den unterschiedlichen Prompts**\n",
    "\n",
    "**--> Es wird ein ver√§nderter Prompt probiert.   \n",
    "Die konkrete Beispiele werden entfernt und nur allgemeine Beschreibung angegeben.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834393b8-90c2-4e21-8e76-46db53f072da",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "prompt = f\"\"\"\n",
    "        Der folgende Text stammt von einem Veranstaltungsplakat. Extrahiere, wenn m√∂glich, die folgenden Informationen:\n",
    "        \n",
    "        - **EVENT** (Art der Veranstaltung)\n",
    "        - **TOPIC** (Thema der Veranstaltung)\n",
    "        - **DATE** (Datumsangaben)\n",
    "        - **TIME** (Uhrzeitangaben)\n",
    "        - **LOC** (Ort der Veranstaltung, z.‚ÄØB. Stra√üen mit Hausnummern, Pl√§tze, Geb√§ude, Stadtteile, St√§dte)\n",
    "        \n",
    "        \n",
    "        Regeln:\n",
    "        - Gib nur Informationen an, bei denen du dir sicher bist.\n",
    "        - Verwende **ausschlie√ülich W√∂rter aus dem Originaltext**.\n",
    "        - Nicht alle Felder m√ºssen vorhanden sein.\n",
    "        - Mehrfache Eintr√§ge pro Feld sind m√∂glich.\n",
    "        \n",
    "        Gib das Ergebnis exakt im folgenden Format zur√ºck:\n",
    "        \n",
    "        ```json\n",
    "        {{\n",
    "          \"EVENT\": [...],\n",
    "          \"TOPIC\": [...],\n",
    "          \"DATE\": [...],\n",
    "          \"TIME\": [...],\n",
    "          \"LOC\": [...]\n",
    "        }}\n",
    "\n",
    "        \n",
    "        Hier ist der Text:\n",
    "        {text}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903830f3-ef5e-4705-8d50-977a2d7301e0",
   "metadata": {},
   "source": [
    "**---> es wurden zwei Durchl√§ufe mit demselben Prompt gemacht**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5597846-0f80-4165-8c92-e0dacecd8b5f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4748e153-8361-45b9-adbc-7d8bf206c092",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ce9bf2aba04ba9b58dfd7807e9ae6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcdf7d7c-97da-4c4d-b266-448fee48c2bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/blongsch/ikt/lib/python3.11/site-packages (4.53.2)\n",
      "Requirement already satisfied: torch in /Users/blongsch/ikt/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: accelerate in /Users/blongsch/ikt/lib/python3.11/site-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/blongsch/ikt/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/blongsch/ikt/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/blongsch/ikt/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/blongsch/ikt/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: sympy in /Users/blongsch/ikt/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/blongsch/ikt/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/blongsch/ikt/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: psutil in /Users/blongsch/ikt/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/blongsch/ikt/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/blongsch/ikt/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/blongsch/ikt/lib/python3.11/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/blongsch/ikt/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/blongsch/ikt/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/blongsch/ikt/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd45a198-18c7-4c66-8ff2-52669c720d19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa1f876ca734bf28b5ff7f24e238d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"cpu\") #habe kein gpu, deshab cpu\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f985c4-93ef-4f37-933e-c11cb9e95c34",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ff503-5752-4ddb-b07a-d6530af0add6",
   "metadata": {},
   "source": [
    "### Auf Datensatz iterieren und Output im Json-Format speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b232fb-d540-427c-b979-cea83f308f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebc91fbbb7d467ca2e913609df9c687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Funktion extract_entities\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "# Model und Pipeline\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "def extract_entities(data):\n",
    "    results = []\n",
    "\n",
    "    for eintrag in data:\n",
    "        file_name = eintrag[\"file_name\"]\n",
    "        text = eintrag[\"text\"]\n",
    "    \n",
    "        prompt = f\"\"\"\n",
    "        Der folgende Text stammt von einem Veranstaltungsplakat. Extrahiere, wenn m√∂glich, die folgenden Informationen:\n",
    "        \n",
    "        - **EVENT** (Art der Veranstaltung)\n",
    "        - **TOPIC** (Thema der Veranstaltung)\n",
    "        - **DATE** (Datumsangaben)\n",
    "        - **TIME** (Uhrzeitangaben)\n",
    "        - **LOC** (Ort der Veranstaltung, z.‚ÄØB. Stra√üen mit Hausnummern, Pl√§tze, Geb√§ude, Stadtteile, St√§dte)\n",
    "        \n",
    "        \n",
    "        Regeln:\n",
    "        - Gib nur Informationen an, bei denen du dir sicher bist.\n",
    "        - Verwende **ausschlie√ülich W√∂rter aus dem Originaltext**.\n",
    "        - Nicht alle Felder m√ºssen vorhanden sein.\n",
    "        - Mehrfache Eintr√§ge pro Feld sind m√∂glich.\n",
    "        \n",
    "        Gib das Ergebnis exakt im folgenden Format zur√ºck:\n",
    "        \n",
    "        ```json\n",
    "        {{\n",
    "          \"EVENT\": [...],\n",
    "          \"TOPIC\": [...],\n",
    "          \"DATE\": [...],\n",
    "          \"TIME\": [...],\n",
    "          \"LOC\": [...]\n",
    "        }}\n",
    "\n",
    "        \n",
    "        Hier ist der Text:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            output = generator(prompt, max_new_tokens=400)[0][\"generated_text\"]\n",
    "            extracted_json = output.split(\"```json\")[-1].split(\"```\")[0].strip()  # nur Output nach \"```json\" wird genommen\n",
    "            extracted = json.loads(extracted_json)\n",
    "        \n",
    "            results.append({\n",
    "                \"file_name\": file_name,\n",
    "                \"entities\": extracted\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_name}: {e}\")\n",
    "            results.append({\n",
    "                \"file_name\": file_name,\n",
    "                \"entities\": None,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31c3154b-f41f-4aac-a369-9e45d79f29c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler bei 0039.jpeg: Invalid control character at: line 2 column 151 (char 152)\n",
      "Fehler bei 0072.jpg: Expecting ',' delimiter: line 7 column 1 (char 174)\n",
      "Fehler bei 0080.jpg: Expecting value: line 2 column 21 (char 22)\n",
      "Fehler bei 0101.jpg: Expecting value: line 2 column 21 (char 22)\n",
      "Fehler bei 0131.jpeg: Expecting property name enclosed in double quotes: line 7 column 1 (char 161)\n",
      "Fehler bei 0140.jpg: Expecting value: line 2 column 21 (char 22)\n",
      "Fehler bei 0176.jpg: Invalid control character at: line 3 column 43 (char 65)\n",
      "Fehler bei 0195.jpg: Expecting value: line 2 column 21 (char 22)\n",
      "Fehler bei 0220.jpg: Expecting ',' delimiter: line 3 column 3 (char 129)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "#sys.path.append(os.path.abspath(\"..\"))\n",
    "#from ressourcen_monitor import monitor\n",
    "\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#@monitor(full_name=\"Gemma2 Prompt1_2 NER\")\n",
    "#def run_ner():\n",
    "#    return extract_entities(data)\n",
    "#results = run_ner()\n",
    "\n",
    "results = extract_entities(data)\n",
    "\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_entity_output_prompt4_2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234991d8-3a7a-412b-9306-f186f9e952b9",
   "metadata": {},
   "source": [
    "### Json-Datei bereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05bb9554-1d0a-4c90-9940-d77456d6d946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamtanzahl Dateien: 200\n",
      "Ung√ºltige Eintr√§ge (keine Entities oder kaputt): 9\n",
      "  ‚îî‚îÄ‚îÄ davon mit 'error'-Feld: 9\n",
      "G√ºltige Eintr√§ge (Entities vorhanden): 191\n"
     ]
    }
   ],
   "source": [
    "# auf \"leere\"/ ung√ºltige Eintr√§ge pr√ºfen\n",
    "import json\n",
    "\n",
    "# JSON laden\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_entity_output_prompt4_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Z√§hler initialisieren\n",
    "gesamt = len(data)\n",
    "ungueltig = 0\n",
    "mit_error = 0\n",
    "gueltig = 0\n",
    "\n",
    "for eintrag in data:\n",
    "    entities = eintrag.get(\"entities\")\n",
    "    \n",
    "    if not entities or not isinstance(entities, dict):\n",
    "        ungueltig += 1\n",
    "        if \"error\" in eintrag:\n",
    "            mit_error += 1\n",
    "    else:\n",
    "        gueltig += 1\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"Gesamtanzahl Dateien: {gesamt}\")\n",
    "print(f\"Ung√ºltige Eintr√§ge (keine Entities oder kaputt): {ungueltig}\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ davon mit 'error'-Feld: {mit_error}\")\n",
    "print(f\"G√ºltige Eintr√§ge (Entities vorhanden): {gueltig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0973154-d5d1-479d-8fd8-642eec02df29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Datei bereingigen / Nulleintr√§ge entfernen\n",
    "import json\n",
    "\n",
    "\n",
    "# JSON laden\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_entity_output_prompt4_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "bereinigt = []\n",
    "for eintrag in data:\n",
    "    entities = eintrag.get(\"entities\")\n",
    "\n",
    "    if not entities or not isinstance(entities, dict):\n",
    "        continue  # √ºberspringen, wenn kaputt oder leer\n",
    "    \n",
    "    # null-Werte rausfiltern \n",
    "    entities_cleaned = {\n",
    "        k: v\n",
    "        for k, v in entities.items()\n",
    "        if v is not None\n",
    "    }\n",
    "\n",
    "    # √ºberspringen, wenn nach dem Filtern nichts √ºbrig ist (optional)\n",
    "    if not entities_cleaned:\n",
    "        continue\n",
    "\n",
    "    eintrag[\"entities\"] = entities_cleaned\n",
    "    bereinigt.append(eintrag)\n",
    "\n",
    "# Speichern\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_entity_output_prompt4_2_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(bereinigt, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62d22ca3-a358-4ead-b324-f47252150a69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Json in Struktur des Goldstandard umwandeln\n",
    "\n",
    "import json\n",
    "\n",
    "# Datei laden\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_entity_output_prompt4_2_cleaned.json\", encoding=\"utf-8\") as f:\n",
    "    gemma_data = json.load(f)\n",
    "\n",
    "# Normalisieren\n",
    "def normalize_to_list(value):\n",
    "    if isinstance(value, list):\n",
    "        result = []\n",
    "        for v in value:\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                result.extend([item.strip() for item in v.split(\",\") if item.strip()])\n",
    "        return result\n",
    "    elif isinstance(value, str) and value.strip():\n",
    "        return [item.strip() for item in value.split(\",\") if item.strip()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# In Goldstandard-√§hnliche Struktur bringen\n",
    "converted = []\n",
    "\n",
    "for entry in gemma_data:\n",
    "    file_name = entry.get(\"file_name\")\n",
    "    raw_entities = entry.get(\"entities\", {})\n",
    "    \n",
    "    entities = []\n",
    "    for label, value in raw_entities.items():\n",
    "        values = normalize_to_list(value)\n",
    "        for v in values:\n",
    "            entities.append({\n",
    "                \"label\": label,\n",
    "                \"text\": v\n",
    "            })\n",
    "    \n",
    "    converted.append({\n",
    "        \"file_name\": file_name,\n",
    "        \"entities\": entities\n",
    "    })\n",
    "\n",
    "# Speichern\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_prompt4_2_as_goldstructure.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(converted, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99551c8-cc29-43d7-a046-7d468d0c8bbd",
   "metadata": {},
   "source": [
    "### Evaluation - 1. Durchlauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff697bd-0eb4-4fde-ad8b-6865177e77e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamtanzahl Dateien: 189\n",
      "Ung√ºltige Eintr√§ge (keine Entities oder kaputt): 0\n",
      "  ‚îî‚îÄ‚îÄ davon mit 'error'-Feld: 0\n",
      "G√ºltige Eintr√§ge (Entities vorhanden): 189\n"
     ]
    }
   ],
   "source": [
    "# auf \"leere\"/ ung√ºltige Eintr√§ge pr√ºfen\n",
    "import json\n",
    "\n",
    "# JSON laden\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_prompt4_1_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Z√§hler initialisieren\n",
    "gesamt = len(data)\n",
    "ungueltig = 0\n",
    "mit_error = 0\n",
    "gueltig = 0\n",
    "\n",
    "for eintrag in data:\n",
    "    entities = eintrag.get(\"entities\")\n",
    "    \n",
    "    if not entities or not isinstance(entities, dict):\n",
    "        ungueltig += 1\n",
    "        if \"error\" in eintrag:\n",
    "            mit_error += 1\n",
    "    else:\n",
    "        gueltig += 1\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"Gesamtanzahl Dateien: {gesamt}\")\n",
    "print(f\"Ung√ºltige Eintr√§ge (keine Entities oder kaputt): {ungueltig}\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ davon mit 'error'-Feld: {mit_error}\")\n",
    "print(f\"G√ºltige Eintr√§ge (Entities vorhanden): {gueltig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df8307b-b863-4403-8b45-6a9e7c1170a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 66666.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Gesamtergebnis (alle Goldstandard-Dateien)\n",
      "Precision: 0.316\n",
      "Recall:    0.222\n",
      "F1-Score:  0.261\n",
      "\n",
      "üìä Metriken pro Kategorie:\n",
      "Label  Precision  Recall  F1-Score  Anzahl in Gold  Anzahl Predicted\n",
      " DATE      0.562   0.345     0.427             290               178\n",
      "EVENT      0.135   0.127     0.130             237               223\n",
      "  LOC      0.291   0.164     0.210             403               227\n",
      " TIME      0.551   0.460     0.501             213               178\n",
      "TOPIC      0.091   0.063     0.074             255               176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# === Dateien laden ===\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_prompt4_1_as_goldstructure.json\", encoding=\"utf-8\") as f:\n",
    "    gemma_data = json.load(f)\n",
    "\n",
    "# === Zu vergleichende Labels ===\n",
    "label_names = [\"LOC\", \"DATE\", \"TIME\", \"EVENT\", \"TOPIC\"]\n",
    "\n",
    "# === Entit√§ten extrahieren ===\n",
    "def extract_entities(entry):\n",
    "    ent_dict = {}\n",
    "    for ent in entry.get(\"entities\", []):\n",
    "        label = ent.get(\"label\")\n",
    "        text = ent.get(\"text\", \"\").strip()\n",
    "        if label and text:\n",
    "            ent_dict.setdefault(label, []).append(text)\n",
    "    return ent_dict\n",
    "\n",
    "# === Mapping der Vorhersagen f√ºr schnellen Zugriff ===\n",
    "gemma_dict = {entry[\"file_name\"]: entry for entry in gemma_data}\n",
    "\n",
    "# === Vergleich vorbereiten ===\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "y_true_per_label = defaultdict(list)\n",
    "y_pred_per_label = defaultdict(list)\n",
    "\n",
    "# === Vergleichsloop √ºber alle Goldstandard-Dateien ===\n",
    "for gold_entry in tqdm(gold_data):\n",
    "    file_name = gold_entry[\"file_name\"]\n",
    "    gold_ents = extract_entities(gold_entry)\n",
    "    pred_entry = gemma_dict.get(file_name)\n",
    "    pred_ents = extract_entities(pred_entry) if pred_entry else {}\n",
    "\n",
    "    for label in label_names:\n",
    "        gold_texts = set(t.strip() for t in gold_ents.get(label, []) if isinstance(t, str))\n",
    "        val = pred_ents.get(label)\n",
    "\n",
    "        if isinstance(val, list):\n",
    "            pred_texts = set(t.strip() for t in val if isinstance(t, str))\n",
    "        elif isinstance(val, str):\n",
    "            pred_texts = {val.strip()}\n",
    "        else:\n",
    "            pred_texts = set()\n",
    "\n",
    "        for text in gold_texts:\n",
    "            if text in pred_texts:\n",
    "                y_true_all.append(1)\n",
    "                y_pred_all.append(1)\n",
    "                y_true_per_label[label].append(1)\n",
    "                y_pred_per_label[label].append(1)\n",
    "                pred_texts.remove(text)\n",
    "            else:\n",
    "                y_true_all.append(1)\n",
    "                y_pred_all.append(0)\n",
    "                y_true_per_label[label].append(1)\n",
    "                y_pred_per_label[label].append(0)\n",
    "\n",
    "        for text in pred_texts:\n",
    "            y_true_all.append(0)\n",
    "            y_pred_all.append(1)\n",
    "            y_true_per_label[label].append(0)\n",
    "            y_pred_per_label[label].append(1)\n",
    "\n",
    "# === Gesamtmetriken ===\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true_all, y_pred_all, average=\"binary\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Gesamtergebnis (alle Goldstandard-Dateien)\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "\n",
    "# === Pro-Label-Metriken ===\n",
    "rows = []\n",
    "for label in label_names:\n",
    "    y_true = y_true_per_label[label]\n",
    "    y_pred = y_pred_per_label[label]\n",
    "\n",
    "    if not y_true and not y_pred:\n",
    "        precision = recall = f1 = 0.0\n",
    "    else:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "\n",
    "    rows.append({\n",
    "        \"Label\": label,\n",
    "        \"Precision\": round(precision, 3),\n",
    "        \"Recall\": round(recall, 3),\n",
    "        \"F1-Score\": round(f1, 3),\n",
    "        \"Anzahl in Gold\": sum(y_true),\n",
    "        \"Anzahl Predicted\": sum(y_pred)\n",
    "    })\n",
    "\n",
    "# Ausgabe\n",
    "df_metrics = pd.DataFrame(rows).sort_values(\"Label\")\n",
    "print(\"\\nüìä Metriken pro Kategorie:\")\n",
    "print(df_metrics.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f721c-39de-4a1e-9516-02f7f57b8013",
   "metadata": {},
   "source": [
    "#### nur Dateien mit erkannten Entit√§ten in die Auswertung mit einbeziehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fe5b71-7398-4390-90f6-a23b1c0f7edd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 189/189 [00:00<00:00, 81463.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Gesamtergebnis (nur gemeinsame Dateien)\n",
      "Precision: 0.316\n",
      "Recall:    0.235\n",
      "F1-Score:  0.270\n",
      "\n",
      "üìä Metriken pro Kategorie:\n",
      "Label  Precision  Recall  F1-Score  Anzahl in Gold  Anzahl Predicted\n",
      " DATE      0.562   0.366     0.443             273               178\n",
      "EVENT      0.135   0.133     0.134             225               223\n",
      "  LOC      0.291   0.175     0.218             378               227\n",
      " TIME      0.551   0.490     0.519             200               178\n",
      "TOPIC      0.091   0.066     0.077             242               176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# === Dateien laden ===\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_prompt4_1_as_goldstructure.json\", encoding=\"utf-8\") as f:\n",
    "    gemma_data = json.load(f)\n",
    "\n",
    "# === Zu vergleichende Labels ===\n",
    "label_names = [\"LOC\", \"DATE\", \"TIME\", \"EVENT\", \"TOPIC\"]\n",
    "\n",
    "# === Entit√§ten extrahieren ===\n",
    "def extract_entities(entry):\n",
    "    ent_dict = {}\n",
    "    for ent in entry.get(\"entities\", []):\n",
    "        label = ent.get(\"label\")\n",
    "        text = ent.get(\"text\", \"\").strip()\n",
    "        if label and text:\n",
    "            ent_dict.setdefault(label, []).append(text)\n",
    "    return ent_dict\n",
    "\n",
    "# === Gemeinsame Dateien filtern ===\n",
    "gold_files = {entry[\"file_name\"] for entry in gold_data}\n",
    "gemma_files = {entry[\"file_name\"] for entry in gemma_data}\n",
    "common_files = gold_files & gemma_files\n",
    "\n",
    "filtered_gold = [entry for entry in gold_data if entry[\"file_name\"] in common_files]\n",
    "gemma_dict = {entry[\"file_name\"]: entry for entry in gemma_data if entry[\"file_name\"] in common_files}\n",
    "\n",
    "# === Vergleich vorbereiten ===\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# pro Label getrennte Z√§hlung\n",
    "y_true_per_label = defaultdict(list)\n",
    "y_pred_per_label = defaultdict(list)\n",
    "\n",
    "# === Vergleichsloop ===\n",
    "for gold_entry in tqdm(filtered_gold):\n",
    "    file_name = gold_entry[\"file_name\"]\n",
    "    gold_ents = extract_entities(gold_entry)\n",
    "    pred_entry = gemma_dict.get(file_name)\n",
    "    pred_ents = extract_entities(pred_entry) if pred_entry else {}\n",
    "\n",
    "    for label in label_names:\n",
    "        gold_texts = set(t.strip() for t in gold_ents.get(label, []) if isinstance(t, str))\n",
    "        val = pred_ents.get(label)\n",
    "\n",
    "        if isinstance(val, list):\n",
    "            pred_texts = set(t.strip() for t in val if isinstance(t, str))\n",
    "        elif isinstance(val, str):\n",
    "            pred_texts = {val.strip()}\n",
    "        else:\n",
    "            pred_texts = set()\n",
    "\n",
    "        # True Positives und False Negatives\n",
    "        for text in gold_texts:\n",
    "            if text in pred_texts:\n",
    "                y_true_all.append(1)\n",
    "                y_pred_all.append(1)\n",
    "                y_true_per_label[label].append(1)\n",
    "                y_pred_per_label[label].append(1)\n",
    "                pred_texts.remove(text)\n",
    "            else:\n",
    "                y_true_all.append(1)\n",
    "                y_pred_all.append(0)\n",
    "                y_true_per_label[label].append(1)\n",
    "                y_pred_per_label[label].append(0)\n",
    "\n",
    "        # Verbleibende Vorhersagen = False Positives\n",
    "        for text in pred_texts:\n",
    "            y_true_all.append(0)\n",
    "            y_pred_all.append(1)\n",
    "            y_true_per_label[label].append(0)\n",
    "            y_pred_per_label[label].append(1)\n",
    "\n",
    "# === Gesamtmetriken ===\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true_all, y_pred_all, average=\"binary\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Gesamtergebnis (nur gemeinsame Dateien)\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "\n",
    "# === Metriken pro Label ===\n",
    "rows = []\n",
    "for label in label_names:\n",
    "    y_true = y_true_per_label[label]\n",
    "    y_pred = y_pred_per_label[label]\n",
    "\n",
    "    if not y_true and not y_pred:\n",
    "        precision = recall = f1 = 0.0\n",
    "    else:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "\n",
    "    rows.append({\n",
    "        \"Label\": label,\n",
    "        \"Precision\": round(precision, 3),\n",
    "        \"Recall\": round(recall, 3),\n",
    "        \"F1-Score\": round(f1, 3),\n",
    "        \"Anzahl in Gold\": sum(y_true),\n",
    "        \"Anzahl Predicted\": sum(y_pred)\n",
    "    })\n",
    "\n",
    "# Ausgabe als Tabelle\n",
    "df_metrics = pd.DataFrame(rows).sort_values(\"Label\")\n",
    "print(\"\\nüìä Metriken pro Kategorie:\")\n",
    "print(df_metrics.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f9546-a8a4-4b54-ad7c-e839a6c095e0",
   "metadata": {},
   "source": [
    "--- \n",
    "### Evaluation - 2. Durchlauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fb582ae-97df-4b36-8b1c-ff10cb645c9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamtanzahl Dateien: 191\n",
      "Ung√ºltige Eintr√§ge (keine Entities oder kaputt): 0\n",
      "  ‚îî‚îÄ‚îÄ davon mit 'error'-Feld: 0\n",
      "G√ºltige Eintr√§ge (Entities vorhanden): 191\n"
     ]
    }
   ],
   "source": [
    "# auf \"leere\"/ ung√ºltige Eintr√§ge pr√ºfen\n",
    "import json\n",
    "\n",
    "# JSON laden\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_entity_output_prompt4_2_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Z√§hler initialisieren\n",
    "gesamt = len(data)\n",
    "ungueltig = 0\n",
    "mit_error = 0\n",
    "gueltig = 0\n",
    "\n",
    "for eintrag in data:\n",
    "    entities = eintrag.get(\"entities\")\n",
    "    \n",
    "    if not entities or not isinstance(entities, dict):\n",
    "        ungueltig += 1\n",
    "        if \"error\" in eintrag:\n",
    "            mit_error += 1\n",
    "    else:\n",
    "        gueltig += 1\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"Gesamtanzahl Dateien: {gesamt}\")\n",
    "print(f\"Ung√ºltige Eintr√§ge (keine Entities oder kaputt): {ungueltig}\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ davon mit 'error'-Feld: {mit_error}\")\n",
    "print(f\"G√ºltige Eintr√§ge (Entities vorhanden): {gueltig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "200a76f1-7059-430b-87eb-12d0e09c1e8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 112674.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Gesamtergebnis 2. Durchlauf (alle Goldstandard-Dateien)\n",
      "Precision: 0.313\n",
      "Recall:    0.222\n",
      "F1-Score:  0.260\n",
      "\n",
      "üìä Metriken pro Kategorie:\n",
      "Label  Precision  Recall  F1-Score  Anzahl in Gold  Anzahl Predicted\n",
      " DATE      0.543   0.348     0.424             290               186\n",
      "EVENT      0.126   0.114     0.119             237               215\n",
      "  LOC      0.284   0.159     0.204             403               225\n",
      " TIME      0.567   0.479     0.519             213               180\n",
      "TOPIC      0.086   0.063     0.073             255               185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# === Dateien laden ===\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_prompt4_2_as_goldstructure.json\", encoding=\"utf-8\") as f:\n",
    "    gemma_data = json.load(f)\n",
    "\n",
    "# === Zu vergleichende Labels ===\n",
    "label_names = [\"LOC\", \"DATE\", \"TIME\", \"EVENT\", \"TOPIC\"]\n",
    "\n",
    "# === Entit√§ten extrahieren ===\n",
    "def extract_entities(entry):\n",
    "    ent_dict = {}\n",
    "    for ent in entry.get(\"entities\", []):\n",
    "        label = ent.get(\"label\")\n",
    "        text = ent.get(\"text\", \"\").strip()\n",
    "        if label and text:\n",
    "            ent_dict.setdefault(label, []).append(text)\n",
    "    return ent_dict\n",
    "\n",
    "# === Mapping der Vorhersagen f√ºr schnellen Zugriff ===\n",
    "gemma_dict = {entry[\"file_name\"]: entry for entry in gemma_data}\n",
    "\n",
    "# === Vergleich vorbereiten ===\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "y_true_per_label = defaultdict(list)\n",
    "y_pred_per_label = defaultdict(list)\n",
    "\n",
    "# === Vergleichsloop √ºber alle Goldstandard-Dateien ===\n",
    "for gold_entry in tqdm(gold_data):\n",
    "    file_name = gold_entry[\"file_name\"]\n",
    "    gold_ents = extract_entities(gold_entry)\n",
    "    pred_entry = gemma_dict.get(file_name)\n",
    "    pred_ents = extract_entities(pred_entry) if pred_entry else {}\n",
    "\n",
    "    for label in label_names:\n",
    "        gold_texts = set(t.strip() for t in gold_ents.get(label, []) if isinstance(t, str))\n",
    "        val = pred_ents.get(label)\n",
    "\n",
    "        if isinstance(val, list):\n",
    "            pred_texts = set(t.strip() for t in val if isinstance(t, str))\n",
    "        elif isinstance(val, str):\n",
    "            pred_texts = {val.strip()}\n",
    "        else:\n",
    "            pred_texts = set()\n",
    "\n",
    "        for text in gold_texts:\n",
    "            if text in pred_texts:\n",
    "                y_true_all.append(1)\n",
    "                y_pred_all.append(1)\n",
    "                y_true_per_label[label].append(1)\n",
    "                y_pred_per_label[label].append(1)\n",
    "                pred_texts.remove(text)\n",
    "            else:\n",
    "                y_true_all.append(1)\n",
    "                y_pred_all.append(0)\n",
    "                y_true_per_label[label].append(1)\n",
    "                y_pred_per_label[label].append(0)\n",
    "\n",
    "        for text in pred_texts:\n",
    "            y_true_all.append(0)\n",
    "            y_pred_all.append(1)\n",
    "            y_true_per_label[label].append(0)\n",
    "            y_pred_per_label[label].append(1)\n",
    "\n",
    "# === Gesamtmetriken ===\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true_all, y_pred_all, average=\"binary\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Gesamtergebnis 2. Durchlauf (alle Goldstandard-Dateien)\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "\n",
    "# === Pro-Label-Metriken ===\n",
    "rows = []\n",
    "for label in label_names:\n",
    "    y_true = y_true_per_label[label]\n",
    "    y_pred = y_pred_per_label[label]\n",
    "\n",
    "    if not y_true and not y_pred:\n",
    "        precision = recall = f1 = 0.0\n",
    "    else:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "\n",
    "    rows.append({\n",
    "        \"Label\": label,\n",
    "        \"Precision\": round(precision, 3),\n",
    "        \"Recall\": round(recall, 3),\n",
    "        \"F1-Score\": round(f1, 3),\n",
    "        \"Anzahl in Gold\": sum(y_true),\n",
    "        \"Anzahl Predicted\": sum(y_pred)\n",
    "    })\n",
    "\n",
    "# Ausgabe\n",
    "df_metrics = pd.DataFrame(rows).sort_values(\"Label\")\n",
    "print(\"\\nüìä Metriken pro Kategorie:\")\n",
    "print(df_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca15638e-cff0-4ddb-a35f-7ac065ca763b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 191/191 [00:00<00:00, 70396.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Gesamtergebnis (nur gemeinsame Dateien)\n",
      "Precision: 0.313\n",
      "Recall:    0.234\n",
      "F1-Score:  0.268\n",
      "\n",
      "üìä Metriken pro Kategorie:\n",
      "Label  Precision  Recall  F1-Score  Anzahl in Gold  Anzahl Predicted\n",
      " DATE      0.543   0.367     0.438             275               186\n",
      "EVENT      0.126   0.120     0.123             225               215\n",
      "  LOC      0.284   0.167     0.211             383               225\n",
      " TIME      0.567   0.505     0.534             202               180\n",
      "TOPIC      0.086   0.066     0.075             241               185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# === Dateien laden ===\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_prompt4_2_as_goldstructure.json\", encoding=\"utf-8\") as f:\n",
    "    gemma_data = json.load(f)\n",
    "\n",
    "# === Zu vergleichende Labels ===\n",
    "label_names = [\"LOC\", \"DATE\", \"TIME\", \"EVENT\", \"TOPIC\"]\n",
    "\n",
    "# === Entit√§ten extrahieren ===\n",
    "def extract_entities(entry):\n",
    "    ent_dict = {}\n",
    "    for ent in entry.get(\"entities\", []):\n",
    "        label = ent.get(\"label\")\n",
    "        text = ent.get(\"text\", \"\").strip()\n",
    "        if label and text:\n",
    "            ent_dict.setdefault(label, []).append(text)\n",
    "    return ent_dict\n",
    "\n",
    "# === Gemeinsame Dateien filtern ===\n",
    "gold_files = {entry[\"file_name\"] for entry in gold_data}\n",
    "gemma_files = {entry[\"file_name\"] for entry in gemma_data}\n",
    "common_files = gold_files & gemma_files\n",
    "\n",
    "filtered_gold = [entry for entry in gold_data if entry[\"file_name\"] in common_files]\n",
    "gemma_dict = {entry[\"file_name\"]: entry for entry in gemma_data if entry[\"file_name\"] in common_files}\n",
    "\n",
    "# === Vergleich vorbereiten ===\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "# pro Label getrennte Z√§hlung\n",
    "y_true_per_label = defaultdict(list)\n",
    "y_pred_per_label = defaultdict(list)\n",
    "\n",
    "# === Vergleichsloop ===\n",
    "for gold_entry in tqdm(filtered_gold):\n",
    "    file_name = gold_entry[\"file_name\"]\n",
    "    gold_ents = extract_entities(gold_entry)\n",
    "    pred_entry = gemma_dict.get(file_name)\n",
    "    pred_ents = extract_entities(pred_entry) if pred_entry else {}\n",
    "\n",
    "    for label in label_names:\n",
    "        gold_texts = set(t.strip() for t in gold_ents.get(label, []) if isinstance(t, str))\n",
    "        val = pred_ents.get(label)\n",
    "\n",
    "        if isinstance(val, list):\n",
    "            pred_texts = set(t.strip() for t in val if isinstance(t, str))\n",
    "        elif isinstance(val, str):\n",
    "            pred_texts = {val.strip()}\n",
    "        else:\n",
    "            pred_texts = set()\n",
    "\n",
    "        # True Positives und False Negatives\n",
    "        for text in gold_texts:\n",
    "            if text in pred_texts:\n",
    "                y_true_all.append(1)\n",
    "                y_pred_all.append(1)\n",
    "                y_true_per_label[label].append(1)\n",
    "                y_pred_per_label[label].append(1)\n",
    "                pred_texts.remove(text)\n",
    "            else:\n",
    "                y_true_all.append(1)\n",
    "                y_pred_all.append(0)\n",
    "                y_true_per_label[label].append(1)\n",
    "                y_pred_per_label[label].append(0)\n",
    "\n",
    "        # Verbleibende Vorhersagen = False Positives\n",
    "        for text in pred_texts:\n",
    "            y_true_all.append(0)\n",
    "            y_pred_all.append(1)\n",
    "            y_true_per_label[label].append(0)\n",
    "            y_pred_per_label[label].append(1)\n",
    "\n",
    "# === Gesamtmetriken ===\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true_all, y_pred_all, average=\"binary\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Gesamtergebnis (nur gemeinsame Dateien)\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "\n",
    "# === Metriken pro Label ===\n",
    "rows = []\n",
    "for label in label_names:\n",
    "    y_true = y_true_per_label[label]\n",
    "    y_pred = y_pred_per_label[label]\n",
    "\n",
    "    if not y_true and not y_pred:\n",
    "        precision = recall = f1 = 0.0\n",
    "    else:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "\n",
    "    rows.append({\n",
    "        \"Label\": label,\n",
    "        \"Precision\": round(precision, 3),\n",
    "        \"Recall\": round(recall, 3),\n",
    "        \"F1-Score\": round(f1, 3),\n",
    "        \"Anzahl in Gold\": sum(y_true),\n",
    "        \"Anzahl Predicted\": sum(y_pred)\n",
    "    })\n",
    "\n",
    "# Ausgabe als Tabelle\n",
    "df_metrics = pd.DataFrame(rows).sort_values(\"Label\")\n",
    "print(\"\\nüìä Metriken pro Kategorie:\")\n",
    "print(df_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d52463-7325-43a3-9265-2a478324dc97",
   "metadata": {},
   "source": [
    "---\n",
    "### Vergleich - √úbereinstimmungen der Durchl√§ufe hinsichtlich JSON Ausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb339903-7349-4a82-b540-38a787825c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Eintr√§ge prompt1: 189\n",
      "Anzahl der Eintr√§ge prompt2: 191\n",
      "Gemeinsame Eintr√§ge (file_name): 183\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_prompt4_1_as_goldstructure.json\", encoding=\"utf-8\") as f:\n",
    "    prompt1 = json.load(f)\n",
    "\n",
    "with open(\"../../data/NER/gemma2/prompt4/gemma2_prompt4_2_as_goldstructure.json\", encoding=\"utf-8\") as f:\n",
    "    prompt2 = json.load(f)\n",
    "\n",
    "# Set der file_names extrahieren\n",
    "file_names1 = {entry['file_name'] for entry in prompt1}\n",
    "file_names2 = {entry['file_name'] for entry in prompt2}\n",
    "\n",
    "\n",
    "gemeinsame = file_names1 & file_names2\n",
    "\n",
    "\n",
    "print(f\"Anzahl der Eintr√§ge prompt1: {len(prompt1)}\")\n",
    "print(f\"Anzahl der Eintr√§ge prompt2: {len(prompt2)}\")\n",
    "print(f\"Gemeinsame Eintr√§ge (file_name): {len(gemeinsame)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb65d4-7910-4311-80aa-f7e2fa1660a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "---> es wurden bei unterschiedlichen Textfiles keine Entit√§ten extrahiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b4f08-2929-49ee-8e8e-9ba16827c82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
