{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8545cf7d-f8d8-4827-8e94-1aa3e6b7aca3",
   "metadata": {},
   "source": [
    "# Flair Evaluation - Optimierung\n",
    "\n",
    "Es wird untersucht wie gut die Entitäten DATE, TIME, EVENT im Datensatz erkannt werden.  \n",
    "Der regelbasierte Ansatz wird zum einen seperat und zum anderen zusammen mit NER durch Flair (nur LOC) evaluiert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b20ade-2a52-4662-8810-c8242dd7d5aa",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1. Evaluation DATE, TIME, EVENT \n",
    "    1.1 alle Entitäten einbeziehen\n",
    "    1.2 ohne Einbeziehung von TOPIC und LOC\n",
    "#### 2. Evaluation Kombination von Regeln und flair\n",
    "    2.1 alle Entitäten einbeziehen\n",
    "    2.2 ohne Einbeziehung von TOPIC\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a5a609-7198-468e-812f-4e7c0728a3a5",
   "metadata": {},
   "source": [
    "### 1. Evaluation DATE, TIME, EVENT \n",
    "#### 1.1 alle Entitäten einbeziehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53492a28-ca6e-48da-809f-81afbd5ef844",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gesamtbewertung ===\n",
      "Precision: 0.68\n",
      "Recall   : 0.34\n",
      "F1-Score : 0.45\n",
      "\n",
      "=== Bewertung pro Label ===\n",
      "DATE       P: 0.58  R: 0.64  F1: 0.61\n",
      "EVENT      P: 0.73  R: 0.55  F1: 0.62\n",
      "LOC        P: 0.00  R: 0.00  F1: 0.00\n",
      "TIME       P: 0.80  R: 0.76  F1: 0.78\n",
      "TOPIC      P: 0.00  R: 0.00  F1: 0.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "GOLD_PATH = \"../../data/data_annotated.json\"\n",
    "PRED_PATH = \"../../data/NER/flair/ner_regex_results.json\"\n",
    "OUTPUT_PATH = \"../../data/NER/flair/ner_optimierung_regex_evaluation.csv\"\n",
    "\n",
    "tp, fp, fn = 0, 0, 0\n",
    "label_stats = defaultdict(lambda: [0, 0, 0])  # TP, FP, FN pro Label\n",
    "file_scores = []\n",
    "\n",
    "with open(GOLD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    gt_data = json.load(f)\n",
    "\n",
    "with open(PRED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "for gt_entry in gt_data:\n",
    "    file_name = gt_entry[\"file_name\"]\n",
    "    gold = gt_entry.get(\"entities\", [])\n",
    "\n",
    "    pred = pred_data.get(file_name)\n",
    "\n",
    "    # Set aus (start, end, label)\n",
    "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in gold} \n",
    "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in pred}\n",
    "    \n",
    "    # TP/FP/FN für Gesamtauswertung\n",
    "    tp_file = len(gold_spans & pred_spans)\n",
    "    fp_file = len(pred_spans - gold_spans)\n",
    "    fn_file = len(gold_spans - pred_spans)\n",
    "\n",
    "    tp += tp_file\n",
    "    fp += fp_file\n",
    "    fn += fn_file\n",
    "\n",
    "    # Pro Datei speichern\n",
    "    precision_file = tp_file / (tp_file + fp_file) if (tp_file + fp_file) > 0 else 0\n",
    "    recall_file = tp_file / (tp_file + fn_file) if (tp_file + fn_file) > 0 else 0\n",
    "    f1_file = 2 * precision_file * recall_file / (precision_file + recall_file) if (precision_file + recall_file) > 0 else 0\n",
    "\n",
    "    file_scores.append({\n",
    "        \"file_name\": file_name,\n",
    "        \"precision\": precision_file,\n",
    "        \"recall\": recall_file,\n",
    "        \"f1_score\": f1_file,\n",
    "        \"tp\": tp_file,\n",
    "        \"fp\": fp_file,\n",
    "        \"fn\": fn_file,\n",
    "    })\n",
    "\n",
    "    # Pro Label\n",
    "    for label in set([e[\"label\"] for e in gold + pred]):\n",
    "        g = {s for s in gold_spans if s[2] == label}\n",
    "        p = {s for s in pred_spans if s[2] == label}\n",
    "        label_stats[label][0] += len(g & p)      # TP\n",
    "        label_stats[label][1] += len(p - g)      # FP\n",
    "        label_stats[label][2] += len(g - p)      # FN\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Gesamtergebnisse\n",
    "# -------------------------\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n=== Gesamtbewertung ===\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall   : {recall:.2f}\")\n",
    "print(f\"F1-Score : {f1:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Bewertung pro Label\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n=== Bewertung pro Label ===\")\n",
    "for label, (tp_l, fp_l, fn_l) in label_stats.items():\n",
    "    p = tp_l / (tp_l + fp_l) if (tp_l + fp_l) > 0 else 0\n",
    "    r = tp_l / (tp_l + fn_l) if (tp_l + fn_l) > 0 else 0\n",
    "    f = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    print(f\"{label:<10} P: {p:.2f}  R: {r:.2f}  F1: {f:.2f}\")\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "df = pd.DataFrame(file_scores)\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075852e-6acc-4048-84c6-33b6760c727b",
   "metadata": {},
   "source": [
    "#### 1.2 Betrachtung ohne TOPIC und LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6f6cf1-a137-4499-81f7-762f3a5485cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gesamtbewertung ===\n",
      "Precision: 0.68\n",
      "Recall   : 0.64\n",
      "F1-Score : 0.66\n",
      "\n",
      "=== Bewertung pro Label ===\n",
      "DATE       P: 0.58  R: 0.64  F1: 0.61\n",
      "EVENT      P: 0.73  R: 0.55  F1: 0.62\n",
      "TIME       P: 0.80  R: 0.76  F1: 0.78\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "GOLD_PATH = \"../../data/data_annotated.json\"\n",
    "PRED_PATH = \"../../data/NER/flair/ner_regex_results.json\"\n",
    "OUTPUT_PATH = \"../../data/NER/flair/ner_optimierung_regex_evaluation_without_topic.csv\"\n",
    "\n",
    "tp, fp, fn = 0, 0, 0\n",
    "label_stats = defaultdict(lambda: [0, 0, 0])  # TP, FP, FN pro Label\n",
    "relevant_labels = {\"TIME\", \"DATE\", \"EVENT\"}\n",
    "file_scores = [] \n",
    "\n",
    "with open(GOLD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    gt_data = json.load(f)\n",
    "\n",
    "with open(PRED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "for gt_entry in gt_data:\n",
    "    file_name = gt_entry[\"file_name\"]\n",
    "    gold = gt_entry.get(\"entities\", [])\n",
    "\n",
    "    pred = pred_data.get(file_name)\n",
    "\n",
    "    # Set aus (start, end, label)\n",
    "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in gold if e[\"label\"] in relevant_labels}\n",
    "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in pred if e[\"label\"] in relevant_labels}\n",
    "    \n",
    "    # TP/FP/FN für Gesamtauswertung\n",
    "    tp_file = len(gold_spans & pred_spans)\n",
    "    fp_file = len(pred_spans - gold_spans)\n",
    "    fn_file = len(gold_spans - pred_spans)\n",
    "\n",
    "    tp += tp_file\n",
    "    fp += fp_file\n",
    "    fn += fn_file\n",
    "\n",
    "    # Pro Datei speichern\n",
    "    precision_file = tp_file / (tp_file + fp_file) if (tp_file + fp_file) > 0 else 0\n",
    "    recall_file = tp_file / (tp_file + fn_file) if (tp_file + fn_file) > 0 else 0\n",
    "    f1_file = 2 * precision_file * recall_file / (precision_file + recall_file) if (precision_file + recall_file) > 0 else 0\n",
    "\n",
    "    file_scores.append({\n",
    "        \"file_name\": file_name,\n",
    "        \"precision\": precision_file,\n",
    "        \"recall\": recall_file,\n",
    "        \"f1_score\": f1_file,\n",
    "        \"tp\": tp_file,\n",
    "        \"fp\": fp_file,\n",
    "        \"fn\": fn_file,\n",
    "    })\n",
    "\n",
    "    # Pro Label\n",
    "    for label in set([e[\"label\"] for e in gold + pred]):\n",
    "        if label not in relevant_labels:\n",
    "            continue \n",
    "        g = {s for s in gold_spans if s[2] == label}\n",
    "        p = {s for s in pred_spans if s[2] == label}\n",
    "        label_stats[label][0] += len(g & p)      # TP\n",
    "        label_stats[label][1] += len(p - g)      # FP\n",
    "        label_stats[label][2] += len(g - p)      # FN\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Gesamtergebnisse\n",
    "# -------------------------\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n=== Gesamtbewertung ===\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall   : {recall:.2f}\")\n",
    "print(f\"F1-Score : {f1:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Bewertung pro Label\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n=== Bewertung pro Label ===\")\n",
    "for label, (tp_l, fp_l, fn_l) in label_stats.items():\n",
    "    p = tp_l / (tp_l + fp_l) if (tp_l + fp_l) > 0 else 0\n",
    "    r = tp_l / (tp_l + fn_l) if (tp_l + fn_l) > 0 else 0\n",
    "    f = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    print(f\"{label:<10} P: {p:.2f}  R: {r:.2f}  F1: {f:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "df = pd.DataFrame(file_scores)\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e564b04-16fd-42ee-a13b-2fdbddd21713",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba361bcf-8718-426e-9481-0620b3d093f4",
   "metadata": {},
   "source": [
    "### 2. Evaluation Kombination von Regeln und Flair\n",
    "#### 2.1 alle Entitäten einbeziehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc4794b-57ab-412d-93fb-4817e192472e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gesamtbewertung ===\n",
      "Precision: 0.57\n",
      "Recall   : 0.47\n",
      "F1-Score : 0.52\n",
      "\n",
      "=== Bewertung pro Label ===\n",
      "DATE       P: 0.58  R: 0.64  F1: 0.61\n",
      "EVENT      P: 0.73  R: 0.55  F1: 0.62\n",
      "LOC        P: 0.40  R: 0.45  F1: 0.42\n",
      "TIME       P: 0.80  R: 0.76  F1: 0.78\n",
      "TOPIC      P: 0.00  R: 0.00  F1: 0.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "GOLD_PATH = \"../../data/data_annotated.json\"\n",
    "PRED_PATH = \"../../data/NER/flair/ner_regex_with_flair_results.json\"\n",
    "OUTPUT_PATH = \"../../data/NER/flair/ner_optimierung_regex_flair_evaluation.csv\"\n",
    "\n",
    "tp, fp, fn = 0, 0, 0\n",
    "label_stats = defaultdict(lambda: [0, 0, 0])  # TP, FP, FN pro Label\n",
    "file_scores = [] \n",
    "\n",
    "\n",
    "with open(GOLD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    gt_data = json.load(f)\n",
    "\n",
    "with open(PRED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "for gt_entry in gt_data:\n",
    "    file_name = gt_entry[\"file_name\"]\n",
    "    gold = gt_entry.get(\"entities\", [])\n",
    "\n",
    "    pred = pred_data.get(file_name)\n",
    "\n",
    "    # Set aus (start, end, label)\n",
    "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in gold} \n",
    "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in pred}\n",
    "    \n",
    "    \n",
    "   # TP/FP/FN für Gesamtauswertung\n",
    "    tp_file = len(gold_spans & pred_spans)\n",
    "    fp_file = len(pred_spans - gold_spans)\n",
    "    fn_file = len(gold_spans - pred_spans)\n",
    "\n",
    "    tp += tp_file\n",
    "    fp += fp_file\n",
    "    fn += fn_file\n",
    "\n",
    "    # Pro Datei speichern\n",
    "    precision_file = tp_file / (tp_file + fp_file) if (tp_file + fp_file) > 0 else 0\n",
    "    recall_file = tp_file / (tp_file + fn_file) if (tp_file + fn_file) > 0 else 0\n",
    "    f1_file = 2 * precision_file * recall_file / (precision_file + recall_file) if (precision_file + recall_file) > 0 else 0\n",
    "\n",
    "    file_scores.append({\n",
    "        \"file_name\": file_name,\n",
    "        \"precision\": precision_file,\n",
    "        \"recall\": recall_file,\n",
    "        \"f1_score\": f1_file,\n",
    "        \"tp\": tp_file,\n",
    "        \"fp\": fp_file,\n",
    "        \"fn\": fn_file,\n",
    "    })\n",
    "\n",
    "    # Pro Label\n",
    "    for label in set([e[\"label\"] for e in gold + pred]):\n",
    "        g = {s for s in gold_spans if s[2] == label}\n",
    "        p = {s for s in pred_spans if s[2] == label}\n",
    "        label_stats[label][0] += len(g & p)      # TP\n",
    "        label_stats[label][1] += len(p - g)      # FP\n",
    "        label_stats[label][2] += len(g - p)      # FN\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Gesamtergebnisse\n",
    "# -------------------------\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n=== Gesamtbewertung ===\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall   : {recall:.2f}\")\n",
    "print(f\"F1-Score : {f1:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Bewertung pro Label\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n=== Bewertung pro Label ===\")\n",
    "for label, (tp_l, fp_l, fn_l) in label_stats.items():\n",
    "    p = tp_l / (tp_l + fp_l) if (tp_l + fp_l) > 0 else 0\n",
    "    r = tp_l / (tp_l + fn_l) if (tp_l + fn_l) > 0 else 0\n",
    "    f = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    print(f\"{label:<10} P: {p:.2f}  R: {r:.2f}  F1: {f:.2f}\")\n",
    "    \n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "df = pd.DataFrame(file_scores)\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd20686-d0ca-4e7a-b5f2-0189107d2a39",
   "metadata": {},
   "source": [
    "#### 2.2 Betrachtung ohne TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c0dc72-dd43-4b75-9d1f-9a116bc8d264",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gesamtbewertung ===\n",
      "Precision: 0.57\n",
      "Recall   : 0.57\n",
      "F1-Score : 0.57\n",
      "\n",
      "=== Bewertung pro Label ===\n",
      "DATE       P: 0.58  R: 0.64  F1: 0.61\n",
      "EVENT      P: 0.73  R: 0.55  F1: 0.62\n",
      "LOC        P: 0.40  R: 0.45  F1: 0.42\n",
      "TIME       P: 0.80  R: 0.76  F1: 0.78\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "GOLD_PATH = \"../../data/data_annotated.json\"\n",
    "PRED_PATH = \"../../data/NER/flair/ner_regex_with_flair_results.json\"\n",
    "OUTPUT_PATH = \"../../data/NER/flair/ner_optimierung_regex_flair_evaluation_without_topic.csv\"\n",
    "\n",
    "tp, fp, fn = 0, 0, 0\n",
    "label_stats = defaultdict(lambda: [0, 0, 0])  # TP, FP, FN pro Label\n",
    "relevant_labels = {\"TIME\", \"DATE\", \"EVENT\",\"LOC\"}\n",
    "file_scores = [] \n",
    "\n",
    "with open(GOLD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    gt_data = json.load(f)\n",
    "\n",
    "with open(PRED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "for gt_entry in gt_data:\n",
    "    file_name = gt_entry[\"file_name\"]\n",
    "    gold = gt_entry.get(\"entities\", [])\n",
    "\n",
    "    pred = pred_data.get(file_name)\n",
    "\n",
    "    # Set aus (start, end, label)\n",
    "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in gold if e[\"label\"] in relevant_labels}\n",
    "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in pred if e[\"label\"] in relevant_labels}\n",
    "    \n",
    "    # TP/FP/FN für Gesamtauswertung\n",
    "    tp_file = len(gold_spans & pred_spans)\n",
    "    fp_file = len(pred_spans - gold_spans)\n",
    "    fn_file = len(gold_spans - pred_spans)\n",
    "\n",
    "    tp += tp_file\n",
    "    fp += fp_file\n",
    "    fn += fn_file\n",
    "\n",
    "    # Pro Datei speichern\n",
    "    precision_file = tp_file / (tp_file + fp_file) if (tp_file + fp_file) > 0 else 0\n",
    "    recall_file = tp_file / (tp_file + fn_file) if (tp_file + fn_file) > 0 else 0\n",
    "    f1_file = 2 * precision_file * recall_file / (precision_file + recall_file) if (precision_file + recall_file) > 0 else 0\n",
    "\n",
    "    file_scores.append({\n",
    "        \"file_name\": file_name,\n",
    "        \"precision\": precision_file,\n",
    "        \"recall\": recall_file,\n",
    "        \"f1_score\": f1_file,\n",
    "        \"tp\": tp_file,\n",
    "        \"fp\": fp_file,\n",
    "        \"fn\": fn_file,\n",
    "    })\n",
    "\n",
    "    # Pro Label\n",
    "    for label in set([e[\"label\"] for e in gold + pred]):\n",
    "        if label not in relevant_labels:\n",
    "            continue \n",
    "        g = {s for s in gold_spans if s[2] == label}\n",
    "        p = {s for s in pred_spans if s[2] == label}\n",
    "        label_stats[label][0] += len(g & p)      # TP\n",
    "        label_stats[label][1] += len(p - g)      # FP\n",
    "        label_stats[label][2] += len(g - p)      # FN\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Gesamtergebnisse\n",
    "# -------------------------\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n=== Gesamtbewertung ===\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall   : {recall:.2f}\")\n",
    "print(f\"F1-Score : {f1:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Bewertung pro Label\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n=== Bewertung pro Label ===\")\n",
    "for label, (tp_l, fp_l, fn_l) in label_stats.items():\n",
    "    p = tp_l / (tp_l + fp_l) if (tp_l + fp_l) > 0 else 0\n",
    "    r = tp_l / (tp_l + fn_l) if (tp_l + fn_l) > 0 else 0\n",
    "    f = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    print(f\"{label:<10} P: {p:.2f}  R: {r:.2f}  F1: {f:.2f}\")\n",
    "    \n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "df = pd.DataFrame(file_scores)\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c3f0f-90c2-4398-896c-50e6a84ee6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
