{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f77e939-a09e-41ed-b80b-0f2c0d5cced4",
   "metadata": {},
   "source": [
    "# Flair Evaluation - 01 -\n",
    "\n",
    "In diesem notebook wird untersucht wie gut Flair die gewünschten Entitäten (EVENT, TOPIC, DATE, TIME, LOC) erkennt.\n",
    "\n",
    "Es wird folgendes Model genutzt: **\"ner_german_large\"**   \n",
    "(large 4-class NER model for German),   \n",
    "F1-Score: 92,31 (CoNLL-03 German revised)  \n",
    "kann vier Entitäten erkennen:  \n",
    "PER, LOC, ORG, MISC  \n",
    "\n",
    "Es wird deshalb erwartet, das nur LOC im Datensatz erkannt wird.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76bf042-c617-44fe-8aa7-755a93d09dec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "### 1. Test auf einem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd22a31a-f4ca-433a-a8f2-7aab169c5321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 13:03:21,363 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, B-PER, E-PER, S-LOC, B-MISC, I-MISC, E-MISC, S-PER, B-ORG, E-ORG, S-ORG, I-ORG, B-LOC, E-LOC, S-MISC, I-PER, I-LOC, <START>, <STOP>\n",
      "Sentence[15]: \"SPRACHCAFE WEIHNACHTSFEIER DIENSTAG 21.12. OLOF-PALME ZENTRUM 19 UHR WIR FREUEN UNS AUF EUCH!\" → [\"WEIHNACHTSFEIER\"/MISC, \"DIENSTAG\"/LOC, \"OLOF-PALME ZENTRUM\"/LOC]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger.load(\"flair/ner-german-large\")   \n",
    "sentence = Sentence(\"SPRACHCAFE WEIHNACHTSFEIER DIENSTAG 21.12. OLOF-PALME ZENTRUM 19 UHR WIR FREUEN UNS AUF EUCH!\")\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c0c427-6b11-4bb4-a8bf-29d953659910",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_Sentence__remove_zero_width_characters', '_Sentence__restore_windows_1252_characters', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_token', '_embeddings', '_handle_problem_characters', '_has_context', '_known_spans', '_metadata', '_next_sentence', '_position_in_dataset', '_previous_sentence', '_printout_labels', '_start_position', 'add_label', 'add_metadata', 'annotation_layers', 'clear_embeddings', 'copy_context_from_sentence', 'embedding', 'end_position', 'get_each_embedding', 'get_embedding', 'get_label', 'get_labels', 'get_language_code', 'get_metadata', 'get_relations', 'get_span', 'get_spans', 'get_token', 'has_label', 'has_metadata', 'infer_space_after', 'is_context_set', 'is_document_boundary', 'labels', 'language_code', 'left_context', 'next_sentence', 'previous_sentence', 'remove_labels', 'right_context', 'score', 'set_context_for_sentences', 'set_embedding', 'set_label', 'start_position', 'tag', 'text', 'to', 'to_dict', 'to_original_text', 'to_plain_string', 'to_tagged_string', 'to_tokenized_string', 'tokenized', 'tokens', 'unlabeled_identifier']\n"
     ]
    }
   ],
   "source": [
    "print(dir(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562189a-b549-4aa3-9c42-3c3fb9630436",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Evaluation auf gesamten Datensatz \n",
    "\n",
    "Es wird nicht auf Token-Ebene (einzelne Wörter), sondern auf Span-Ebene untersucht, also auf zusammenhängende Entitäten mit ihrer jeweiligen Start und End Position im Text. Konkret: Start - Endpostion, Label und Text müssen bei NER und Goldstandard exakt gleich sein.  \n",
    "Es werden nur die relevanten Labels EVENT, TOPIC, DATE, TIME und LOC betrachtet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087bb0f3-eb54-4eb9-8f75-e1ec11750a29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Importieren des annotierten Datensatzes\n",
    "import json\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    all_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29825fdd-c61d-4594-9489-8610a0a419d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 17:36:02,862 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, B-PER, E-PER, S-LOC, B-MISC, I-MISC, E-MISC, S-PER, B-ORG, E-ORG, S-ORG, I-ORG, B-LOC, E-LOC, S-MISC, I-PER, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "# Funktion zur Extraktion der Entitäten aus dem Text durch Flair // \"get_spans(\"ner\")\" gibt text, Start- und Endposition und Label aus\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger.load(\"flair/ner-german-large\")  \n",
    "\n",
    "def extract_flair_entities(text):\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    predicted = []\n",
    "\n",
    "    for entity in sentence.get_spans(\"ner\"):\n",
    "        predicted.append({\n",
    "            \"text\": entity.text,\n",
    "            \"start\": entity.start_position,\n",
    "            \"end\": entity.end_position,\n",
    "            \"label\": entity.get_label(\"ner\").value\n",
    "        })\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a7fe4d4-8a1f-4a06-a086-5f9b71873741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gesamtbewertung ===\n",
      "Precision: 0.41\n",
      "Recall   : 0.13\n",
      "F1-Score : 0.20\n",
      "\n",
      "=== Bewertung pro Label ===\n",
      "LOC        P: 0.41  R: 0.45  F1: 0.43\n",
      "TIME       P: 0.00  R: 0.00  F1: 0.00\n",
      "DATE       P: 0.00  R: 0.00  F1: 0.00\n",
      "EVENT      P: 0.00  R: 0.00  F1: 0.00\n",
      "TOPIC      P: 0.00  R: 0.00  F1: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Iteration auf dem gesamten Datensatz, Berechnung der Metriken, Speichern des NER Outputs im im Json\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "# Funktion zur erstellung des dictionary\n",
    "def span_to_dict(span, text):\n",
    "    return {\n",
    "        \"text\": text[span[0]:span[1]],\n",
    "        \"start\": span[0],\n",
    "        \"end\": span[1],\n",
    "        \"label\": span[2]\n",
    "    }\n",
    "\n",
    "\n",
    "tp, fp, fn = 0, 0, 0\n",
    "label_stats = defaultdict(lambda: [0, 0, 0])  # TP, FP, FN pro Label\n",
    "relevant_labels = {\"EVENT\", \"TOPIC\", \"TIME\", \"DATE\", \"LOC\"}\n",
    "all_results = []\n",
    "\n",
    "for eintrag in all_data:\n",
    "    file_name = eintrag.get(\"file_name\", None)\n",
    "    text = eintrag[\"text\"]\n",
    "    gold = eintrag.get(\"entities\", [])\n",
    "    predicted = extract_flair_entities(text)\n",
    "\n",
    "    # Set aus (start, end, label)\n",
    "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in gold if e[\"label\"] in relevant_labels}\n",
    "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in predicted if e[\"label\"] in relevant_labels}\n",
    "\n",
    "    # Gesamtmetriken\n",
    "    tp += len(gold_spans & pred_spans)\n",
    "    fp += len(pred_spans - gold_spans)\n",
    "    fn += len(gold_spans - pred_spans)\n",
    "\n",
    "    # für Berechnung pro Eintrag\n",
    "    tp_spans = gold_spans & pred_spans\n",
    "    fp_spans = pred_spans - gold_spans\n",
    "    fn_spans = gold_spans - pred_spans\n",
    "    \n",
    "    tp_count = len(tp_spans)\n",
    "    fp_count = len(fp_spans)\n",
    "    fn_count = len(fn_spans)\n",
    "    \n",
    "    # lokale Metriken für dieses Dokument berechnen\n",
    "    precision_local = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "    recall_local = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "    f1_local = 2 * precision_local * recall_local / (precision_local + recall_local) if (precision_local + recall_local) > 0 else 0\n",
    "    \n",
    "\n",
    "    # Pro Label\n",
    "    for label in set([e[\"label\"] for e in gold + predicted]):\n",
    "        if label not in relevant_labels:\n",
    "            continue \n",
    "    \n",
    "        g = {s for s in gold_spans if s[2] == label}\n",
    "        p = {s for s in pred_spans if s[2] == label}\n",
    "        label_stats[label][0] += len(g & p)      # TP\n",
    "        label_stats[label][1] += len(p - g)      # FP\n",
    "        label_stats[label][2] += len(g - p)      # FN\n",
    "\n",
    "    result = {\n",
    "    \"file_name\": file_name,\n",
    "    \"text\": text,\n",
    "    \"precision\": precision_local,\n",
    "    \"recall\": recall_local,\n",
    "    \"f1\": f1_local,\n",
    "    \"true_positives\": [span_to_dict(s, text) for s in tp_spans],\n",
    "    \"false_positives\": [span_to_dict(s, text) for s in fp_spans],\n",
    "    \"false_negatives\": [span_to_dict(s, text) for s in fn_spans],\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "# Speichern der results / Ergebnis pro Eintrag\n",
    "with open(\"../../data/NER/flair/results_ner_german_large.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. Gesamtergebnisse\n",
    "# -------------------------\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n=== Gesamtbewertung ===\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall   : {recall:.2f}\")\n",
    "print(f\"F1-Score : {f1:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Bewertung pro Label\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n=== Bewertung pro Label ===\")\n",
    "for label, (tp_l, fp_l, fn_l) in label_stats.items():\n",
    "    p = tp_l / (tp_l + fp_l) if (tp_l + fp_l) > 0 else 0\n",
    "    r = tp_l / (tp_l + fn_l) if (tp_l + fn_l) > 0 else 0\n",
    "    f = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    print(f\"{label:<10} P: {p:.2f}  R: {r:.2f}  F1: {f:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99578c04-41ba-4490-b4a6-16abb5e45e16",
   "metadata": {},
   "source": [
    "---> wie erwartet wird außer LOC keine andere Entität erkannt  \n",
    "---> wenn man nur die Werte von LOC betrachtet, immer noch weit entfernt vom propagierten F1-Score 92,31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fa22a-911c-407c-b4fe-0d9eb9a6e0dd",
   "metadata": {},
   "source": [
    "**eine mögliche Ursachen für schlechten  Wert:** \n",
    "- unterschiedliche Start und End Positionen bei vorhergesagten und Goldstandard Entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac14396b-11c8-4861-801f-dddefe278dff",
   "metadata": {},
   "source": [
    "---\n",
    "#### Untersuchung, ob unterschiedliche Start- und Endpositionen bei predicted und Goldstandard Ursache für schlechte Werte sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d21c2dd-d9e5-49b6-9486-5e9a6e6215f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOC-Ergebnisse ===\n",
      "True Positives (genau): 188\n",
      "Overlap Matches (Text & Label gleich): 191\n",
      "Fuzzy Matches (±2 Zeichen): 198\n",
      "Fuzzy Matches ohne Overlaps: 7\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "with open(\"../../data/NER/flair/results_ner_german_large.json\", encoding=\"utf-8\") as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "def span_text(span, text):\n",
    "    return text[span[0]:span[1]]\n",
    "\n",
    "def fuzzy_match(gold_span, pred_span, tolerance=2):\n",
    "    return (\n",
    "        gold_span[2] == pred_span[2] and\n",
    "        abs(gold_span[0] - pred_span[0]) <= tolerance and\n",
    "        abs(gold_span[1] - pred_span[1]) <= tolerance\n",
    "    )\n",
    "\n",
    "# Index predicted nach file_name\n",
    "pred_index = {entry[\"file_name\"]: entry for entry in pred_data}\n",
    "\n",
    "\n",
    "tp = 0\n",
    "overlap_matches = 0  # nur text und label sind gleich\n",
    "fuzzy_matches = 0    # text und label sind gleich und position weicht um maximal 2 Zeichen ab\n",
    "\n",
    "for eintrag in gold_data:\n",
    "    file_name = eintrag.get(\"file_name\")\n",
    "    text = eintrag[\"text\"]\n",
    "    gold = [e for e in eintrag.get(\"entities\", []) if e[\"label\"] == \"LOC\"]\n",
    "\n",
    "    pred_entry = pred_index.get(file_name, {})\n",
    "    # Kombiniere TP und FP, FN interessieren hier nicht\n",
    "    predicted = pred_entry.get(\"true_positives\", []) + pred_entry.get(\"false_positives\", [])\n",
    "    predicted = [e for e in predicted if e[\"label\"] == \"LOC\"]\n",
    "\n",
    "    # Erstelle Sets für TP-Zählung\n",
    "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in gold}\n",
    "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in predicted}\n",
    "\n",
    "    tp += len(gold_spans & pred_spans)\n",
    "\n",
    "    gold_spans_list = list(gold_spans)\n",
    "    pred_spans_list = list(pred_spans)\n",
    "\n",
    "    # Overlap: label und Text gleich\n",
    "    matched_pred_indices = set()\n",
    "    for g in gold_spans:\n",
    "        for i, p in enumerate(pred_spans_list):\n",
    "            if i in matched_pred_indices:\n",
    "                continue\n",
    "            if g[2] == p[2] and span_text(g, text) == span_text(p, text):\n",
    "                overlap_matches += 1\n",
    "                matched_pred_indices.add(i)\n",
    "                break\n",
    "\n",
    "    # Fuzzy Match: ±2 Zeichen Toleranz bei Start/Ende\n",
    "    matched_pred_indices_fuzzy = set()\n",
    "    for g in gold_spans_list:\n",
    "        for i, p in enumerate(pred_spans_list):\n",
    "            if i in matched_pred_indices_fuzzy:\n",
    "                continue\n",
    "            if fuzzy_match(g, p):\n",
    "                fuzzy_matches += 1\n",
    "                matched_pred_indices_fuzzy.add(i)\n",
    "                break\n",
    "\n",
    "print(f\"\\n=== LOC-Ergebnisse ===\")\n",
    "print(f\"True Positives (genau): {tp}\")\n",
    "print(f\"Overlap Matches (Text & Label gleich): {overlap_matches}\")\n",
    "print(f\"Fuzzy Matches (±2 Zeichen): {fuzzy_matches}\")\n",
    "print(f\"Fuzzy Matches ohne Overlaps: {fuzzy_matches - overlap_matches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b13450-d63a-40ba-9d23-b44bbc117db3",
   "metadata": {},
   "source": [
    "Insgesamt wurden 198 LOC-Entitäten als fuzzy matches erkannt, d. h. mit korrekt erkanntem Label und maximal ±2 Zeichen Positionsabweichung. Davon waren 188 True Positives, also exakt richtig positioniert.  \n",
    "\n",
    "Das bedeutet: 10 LOCs wurden inhaltlich korrekt erkannt, aber nicht exakt an der richtigen Position. Diese lagen entweder knapp daneben (innerhalb der Fuzzy-Toleranz) oder hatten nur den richtigen Text (Overlap), aber eine größere Positionsabweichung.  \n",
    "\n",
    "Da sich Overlap- und Fuzzy-Matches überschneiden können, lässt sich nicht eindeutig sagen, ob die 3 zusätzlichen Overlap-Matches in den 10 fuzzy-only Fällen enthalten sind oder ganz andere Treffer mit falscher Position darstellen.\n",
    "\n",
    "\n",
    "---> Da 10 erkannte Entitäten mit falscher Position eine relativ kleine Anzahl ist die schlechte Vorhersage von LOC nicht primär durch unterschiedlichen Positionen von Goldstandard und prediction begründet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9176c-7884-4b9f-97fd-f140bcc83bc8",
   "metadata": {},
   "source": [
    "---> zum Vergleich: wie viele LOC Entitäten sind im gold standard vorhanden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38d2488-67d3-4f51-af2c-03c6c4318ca4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl LOC: 415\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "count = 0\n",
    "for item in data:\n",
    "    for entity in item[\"entities\"]:\n",
    "        if entity[\"label\"] == \"LOC\":\n",
    "            count += 1\n",
    "\n",
    "print(f\"Anzahl LOC: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16972991-f554-4411-90d2-1d5c49bb5539",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### Metriken berechnen für alle Entitäten außer TOPIC (für Vergleich mit regelbasierter optimierter Variante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af494be3-6623-4f45-b286-a16613771ead",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gesamtbewertung (nur relevante Labels) ===\n",
      "Precision: 0.41\n",
      "Recall   : 0.16\n",
      "F1-Score : 0.23\n",
      "\n",
      "=== Bewertung pro Label ===\n",
      "DATE       P: 0.00  R: 0.00  F1: 0.00\n",
      "LOC        P: 0.41  R: 0.45  F1: 0.43\n",
      "TIME       P: 0.00  R: 0.00  F1: 0.00\n",
      "EVENT      P: 0.00  R: 0.00  F1: 0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "with open(\"../../data/NER/flair/results_ner_german_large.json\", encoding=\"utf-8\") as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "with open(\"../../data/data_annotated.json\", encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "# Index predicted nach file_name\n",
    "pred_index = {entry[\"file_name\"]: entry for entry in pred_data}\n",
    "\n",
    "relevant_labels = {\"EVENT\", \"TIME\", \"DATE\", \"LOC\"}\n",
    "\n",
    "# Initialisierung\n",
    "tp, fp, fn = 0, 0, 0\n",
    "label_stats = defaultdict(lambda: [0, 0, 0])  # TP, FP, FN pro Label\n",
    "\n",
    "for eintrag in gold_data:\n",
    "    file_name = eintrag.get(\"file_name\")\n",
    "    text = eintrag[\"text\"]\n",
    "    gold = [e for e in eintrag.get(\"entities\", []) if e[\"label\"] in relevant_labels]\n",
    "\n",
    "    pred_entry = pred_index.get(file_name, {})\n",
    "    predicted = pred_entry.get(\"true_positives\", []) + pred_entry.get(\"false_positives\", [])\n",
    "\n",
    "    predicted = [e for e in predicted if e[\"label\"] in relevant_labels]\n",
    "\n",
    "    gold_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in gold}\n",
    "    pred_spans = {(e[\"start\"], e[\"end\"], e[\"label\"]) for e in predicted}\n",
    "\n",
    "    tp_set = gold_spans & pred_spans\n",
    "    fp_set = pred_spans - gold_spans\n",
    "    fn_set = gold_spans - pred_spans\n",
    "\n",
    "    tp += len(tp_set)\n",
    "    fp += len(fp_set)\n",
    "    fn += len(fn_set)\n",
    "\n",
    "    for label in relevant_labels:\n",
    "        g = {s for s in gold_spans if s[2] == label}\n",
    "        p = {s for s in pred_spans if s[2] == label}\n",
    "        label_stats[label][0] += len(g & p)\n",
    "        label_stats[label][1] += len(p - g)\n",
    "        label_stats[label][2] += len(g - p)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Gesamtergebnisse\n",
    "# -------------------------\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n=== Gesamtbewertung (nur relevante Labels) ===\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall   : {recall:.2f}\")\n",
    "print(f\"F1-Score : {f1:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Bewertung pro Label\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n=== Bewertung pro Label ===\")\n",
    "for label, (tp_l, fp_l, fn_l) in label_stats.items():\n",
    "    p = tp_l / (tp_l + fp_l) if (tp_l + fp_l) > 0 else 0\n",
    "    r = tp_l / (tp_l + fn_l) if (tp_l + fn_l) > 0 else 0\n",
    "    f = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    print(f\"{label:<10} P: {p:.2f}  R: {r:.2f}  F1: {f:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b89d5-b2a9-4f5d-81e4-bb0201e7504b",
   "metadata": {},
   "source": [
    "---\n",
    "### Diagramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b24ccc-1da5-4c82-9703-9ba0e2290c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# JSON in CSV wandeln und speichern\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# JSON-Datei laden\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m../../data/NER/flair/results_ner_german_large.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# JSON in CSV wandeln und speichern\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# JSON-Datei laden\n",
    "with open(\"../../data/NER/flair/results_ner_german_large.json\", \"r\") as f:\n",
    "    data = json.load(f)  \n",
    "\n",
    "# Nur gewünschte Felder extrahieren\n",
    "filtered_data = [\n",
    "    {\n",
    "        \"file_name\": entry[\"file_name\"],\n",
    "        \"precision\": entry[\"precision\"],\n",
    "        \"recall\": entry[\"recall\"],\n",
    "        \"f1\": entry[\"f1\"]\n",
    "    }\n",
    "    for entry in data\n",
    "]\n",
    "\n",
    "# In DataFrame und CSV schreiben\n",
    "df = pd.DataFrame(filtered_data)\n",
    "df.to_csv(\"../../data/NER/flair/results_ner_german_large.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da21ecf7-a9d6-4186-87a7-cb7970a0e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/blongsch/ikt/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/blongsch/ikt/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/blongsch/ikt/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/blongsch/ikt/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/blongsch/ikt/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/blongsch/ikt/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed27e73-4b4e-4ec6-a3b3-c90bcb43c191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Flair 3.11)",
   "language": "python",
   "name": "flair311_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
