{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bea079-5e87-4daa-8d45-683844932598",
   "metadata": {},
   "source": [
    "## Umwandlung der annotierten Daten ins BIO-Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6963214-b96e-4547-9984-2ef295b6fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "def convert_to_conll_span_tokenizer(json_data):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    dataset = []\n",
    "\n",
    "    for item in json_data:\n",
    "        text = item['text']\n",
    "        entities = item['entities']\n",
    "        \n",
    "        # Liste der Entity-Spans\n",
    "        spans = []\n",
    "        for ent in entities:\n",
    "            start, end, label = ent['start'], ent['end'], ent['label']\n",
    "            if start >= len(text) or end > len(text) or start >= end:\n",
    "                continue\n",
    "            spans.append((start, end, label))\n",
    "\n",
    "        # Tokenize mit Start/End\n",
    "        tokens = list(tokenizer.span_tokenize(text))\n",
    "        conll_sample = []\n",
    "\n",
    "        for start, end in tokens:\n",
    "            token = text[start:end]\n",
    "            tag = 'O'\n",
    "\n",
    "            for ent_start, ent_end, label in spans:\n",
    "                if start == ent_start:\n",
    "                    tag = f'B-{label}'\n",
    "                    break\n",
    "                elif ent_start < start < ent_end:\n",
    "                    tag = f'I-{label}'\n",
    "                    break\n",
    "\n",
    "            conll_sample.append((token, tag))\n",
    "\n",
    "        dataset.append(conll_sample)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def save_to_conll(dataset, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for sample in dataset:\n",
    "            for token, tag in sample:\n",
    "                f.write(f\"{token} {tag}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Laden und ausfÃ¼hren\n",
    "with open('../../data/data_annotated.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "conll_data = convert_to_conll_span_tokenizer(data)\n",
    "save_to_conll(conll_data, '../../data/output_data.conll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22c655-bd30-4e1b-8672-51ed3b76d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Flair 3.11)",
   "language": "python",
   "name": "flair311_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
