{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16b9c2e-40d3-4b7b-945a-bd15e90631f5",
   "metadata": {},
   "source": [
    "# Flair - Optimierung durch regelbasierte Entitätserkennung\n",
    "\n",
    "\n",
    "Da Flair von den gewünschten Entitäten nur LOC erkennt, soll eine regelbasierte Erkennung weiterer Entitäten (DATE, TIME, EVENT) vorgenommen werden.  \n",
    "In diesem notebook werden diese Regeln definiert. \n",
    "\n",
    "Die Kombination aus Flair und regelbasierter Entitätserkennung wird auf dem Datensatz angewendet und in die Ergebnisse zur weiteren Untersuchung einem Json gespeichert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19882ac7-1233-49bc-aafc-ea72ee0c4614",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1. Regeln  \n",
    "    1.1 DATE  \n",
    "    1.2 TIME  \n",
    "    1.3 EVENT  \n",
    "#### 2. Anwendung auf Datensatz\n",
    "    2.1 nur regelbasierte Erkennung \n",
    "    2.2 in Kombination mit Flair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37435457-5217-4880-a85b-cabca6c324db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b956d68-9a50-4aa6-8a6a-09e1f5144621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-17 18:30:44,902 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, B-PER, E-PER, S-LOC, B-MISC, I-MISC, E-MISC, S-PER, B-ORG, E-ORG, S-ORG, I-ORG, B-LOC, E-LOC, S-MISC, I-PER, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger.load(\"flair/ner-german-large\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57358f-c043-4a72-aaf0-c90a884052cf",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Regeln\n",
    "#### 1.1 Regeln für DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8a65e0-3376-4d8b-8057-6d99b22e62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "date_pattern = re.compile(r'''\n",
    "(\n",
    "    # Formate wie 12.07.2023 oder 12.07. (mit Punkt nach dem Monat!)\n",
    "    (?<!\\d)\n",
    "    (?:\n",
    "        (?:0?[1-9]|[12][0-9]|3[01])     # Tag\n",
    "        \\s*\\.\\s*\n",
    "        (?:0?[1-9]|1[0-2])              # Monat\n",
    "        \\s*\\.\\s*\n",
    "        (?:\\d{2,4})?                    # optionales Jahr\n",
    "    )\n",
    "    (?!\\d)\n",
    "\n",
    "    |\n",
    "    # Formate wie 12.07. - 14.07. 2023 (mit Punkten nach dem Monat)\n",
    "    (?<!\\d)\n",
    "    (?:\n",
    "        (?:0?[1-9]|[12][0-9]|3[01])\n",
    "        \\s*\\.\\s*\n",
    "        (?:0?[1-9]|1[0-2])\n",
    "        \\s*\\.\\s*\n",
    "        -\\s*\n",
    "        (?:0?[1-9]|[12][0-9]|3[01])\n",
    "        \\s*\\.\\s*\n",
    "        (?:0?[1-9]|1[0-2])\n",
    "        \\s*\\.\\s*\n",
    "        \\d{2,4}\n",
    "    )\n",
    "    (?!\\d)\n",
    "\n",
    "    |\n",
    "    # Format wie 13. August oder 13. August 2023\n",
    "    (?<!\\d)\n",
    "    (?:\n",
    "        (?:0?[1-9]|[12][0-9]|3[01])              # Tag\n",
    "        \\s*\\.\\s*\n",
    "        (?:Januar|Februar|März|April|Mai|Juni|Juli|\n",
    "           August|September|Oktober|November|Dezember)   # Monat ausgeschrieben\n",
    "        (?:\\s+\\d{2,4})?                          # optionales Jahr\n",
    "    )\n",
    "    (?!\\d)\n",
    "    \n",
    "\n",
    "    |\n",
    "    # Format wie 1-1-2024\n",
    "    (?<!\\d)\n",
    "    (?:\n",
    "        (?:0?[1-9]|[12][0-9]|3[01])\n",
    "        \\s*-\\s*\n",
    "        (?:0?[1-9]|1[0-2])\n",
    "        \\s*-\\s*\n",
    "        \\d{2,4}\n",
    "    )\n",
    "    (?!\\d)\n",
    "\n",
    "    |\n",
    "    # Wochentage\n",
    "    (?<=\\s)(?:Mo|Di|Mi|Do|Fr|Sa|So)\\.?(?=\\s) |\n",
    "    (?<=\\s)(?:mo|di|mi|do|fr|sa|so)\\.?(?=\\s) |\n",
    "    (?:Montag|Dienstag|Mittwoch|Donnerstag|Freitag|Samstag|Sonntag|Sonnabend)\n",
    "\n",
    "    |\n",
    "    # Monate\n",
    "    (?:Januar|Februar|März|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember) |\n",
    "    (?:Jan|Feb|Mär|Apr|Mai|Jun|Jul|Aug|Sep|Okt|Nov|Dez)\\.?\n",
    "\n",
    "    |\n",
    "    # relative Begriffe\n",
    "    \\b(?:morgen|heute|abend|Wochenende|WE)\\b\n",
    "\n",
    "    |\n",
    "    # Erweiterte Kombinationen:\n",
    "    \\b(?:jeden|immer|nächsten|letzten|letztes|nächstes)\\s+\n",
    "    (?:\n",
    "        Montag|Dienstag|Mittwoch|Donnerstag|Freitag|Samstag|Sonntag|Sonnabend|\n",
    "        Montags|Dienstags|Mittwochs|Donnerstags|Freitags|Samstags|Sonntags|\n",
    "        Wochenende\n",
    "    )\n",
    ")\n",
    "''', re.IGNORECASE | re.VERBOSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dcd38d-c845-471c-b1fe-bd2249c50748",
   "metadata": {},
   "source": [
    "#### 1.2 Regeln für TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a25351-34fd-4fea-a893-de31124f6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "time_pattern = re.compile(r'''\n",
    "    \\b\n",
    "    (                                           # Gruppe für die ganze Uhrzeit\n",
    "        (?:                                     # Entweder:\n",
    "            \\d{1,2}                             # Stunde\n",
    "            [:\\-–]                              # Trenner: :, -, –\n",
    "            \\d{2}                               # Minuten\n",
    "            (?:\\s?(?:Uhr|h))?                   # Optional \"Uhr\"/\"h\"\n",
    "        |\n",
    "            \\d{1,2}                             # Nur Stunde\n",
    "            \\s?(?:Uhr|h)                        # Aber **nur**, wenn \"Uhr\"/\"h\" folgt\n",
    "        )\n",
    "        (?:\\s?[–-]\\s?                           # Optionaler Bereich (z.B. \" - \")\n",
    "            (?:\\d{1,2}\n",
    "                (?:[:.]\\d{2})?\n",
    "            )\n",
    "            (?:\\s?(?:Uhr|h))?\n",
    "        )?\n",
    "    )\n",
    "    \\b\n",
    "''', re.IGNORECASE | re.VERBOSE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac41dd-f595-47bf-a4c3-a1eb4d6db997",
   "metadata": {},
   "source": [
    "#### 1.3 Regeln für EVENT\n",
    "es werden einzelne Worte, die typisch sind für Veranstaltungen definiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba1f5d58-86d2-42cc-a997-a52baf3372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Gruppe 1: Begriffe mit möglichem Präfix (z. B. Montagsdemo)\n",
    "prefix_allowed = [\"Demo\", \"Demonstration\", \"Kundgebung\", \"Gedenken\", \"Lesung\", \"Feier\", \"Spaziergang\", \"Fest\", \"Party\", \"Protest\", \"Abend\"]\n",
    "\n",
    "# Gruppe 2: Begriffe mit möglichem Suffix (z. B. Protesttag, Aktionswoche)\n",
    "suffix_allowed = [\"Protest\", \"Aktion\", ]\n",
    "\n",
    "# Gruppe 3: Nur als eigenständiges Wort zulässig\n",
    "exact_only = [\n",
    "    \"Party\", \"Konzert\", \"Filmabend\", \"Aufführung\", \"Jubiläum\", \"Veranstaltung\",\n",
    "    \"Vorführung\", \"Vortrag\", \"Flohmarkt\", \"Picknick\", \"Infotag\", \"Umzug\", \"Aufzug\",\n",
    "    \"Fest\", \"Mahnwache\", \"Austellung\", \"Treffen\", \"Konferenz\"\n",
    "]\n",
    "\n",
    "# Pattern bauen\n",
    "prefix_pattern = r\"\\b(?:[\\wäöüÄÖÜß]+-?)*(?:{})\\b\".format(\"|\".join(prefix_allowed))\n",
    "suffix_pattern = r\"\\b(?:{})[\\wäöüÄÖÜß]*\\b\".format(\"|\".join(suffix_allowed))\n",
    "exact_pattern = r\"\\b(?:{})\\b\".format(\"|\".join(exact_only))\n",
    "\n",
    "# Gesamtpattern\n",
    "event_pattern = r\"{}|{}|{}\".format(prefix_pattern, suffix_pattern, exact_pattern)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30879144-120e-40a3-9474-04b4d56f39d9",
   "metadata": {},
   "source": [
    "**TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e2f2d5-bf78-41ae-959f-3b501e910567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sonnabend', '13. August', '4.Juli 24', 'Dienstag', '10.10. ', '18.06.2025', 'Januar', 'Jul', 'heute', 'morgen', '12.12. 2011']\n",
      "['10 Uhr', '19-20 Uhr', '14:30 Uhr']\n",
      "['Sommerfest', 'Sonnabend', 'Demo', 'FEIER', 'Party', 'Frühlingsparty', 'Soli-Party']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Es gab Sommerfest Sonnabend eine Demo und eine FEIER Film gestern. 13. August 4.Juli 24 Jeden Party Frühlingsparty  Soli-Party Dienstag 10.10. um 10 Uhr Das 18.30 19-20 Uhr am 18.06.2025 im Januar und im Jul heute und dann gehen wir morgen Sonne 12.12. 2011 villeicht 8/8/24 um 14:30 Uhr statt.\"\n",
    "dates = re.findall(date_pattern, text)\n",
    "times = re.findall(time_pattern, text)\n",
    "print(dates)\n",
    "print(times)\n",
    "\n",
    "events = re.findall(event_pattern, text, flags=re.IGNORECASE)\n",
    "print(events)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b4fc8-a2d4-40a8-b84f-a66b77a968c0",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Anwenden\n",
    "#### 2.1 auschließlich regelbasierte Entitätserkennung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a60b88-1e81-43fb-b7ac-f4a361babd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "JSON_PATH = \"../../data/data_annotated.json\"\n",
    "OUTPUT_FILE = \"../../data/NER/flair/ner_regex_results.json\"\n",
    "\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "results = {} #dictionary\n",
    "\n",
    "for eintrag in data:\n",
    "    file_name = eintrag.get(\"file_name\")\n",
    "    if not file_name:\n",
    "        continue\n",
    "\n",
    "    ground_truth = eintrag.get(\"text\", \"\") \n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    for m in re.finditer(time_pattern, ground_truth):\n",
    "        annotations.append({\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "            \"label\": \"TIME\",\n",
    "            \"text\": m.group()\n",
    "        })\n",
    "\n",
    "    for m in re.finditer(date_pattern, ground_truth):\n",
    "        annotations.append({\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "            \"label\": \"DATE\",\n",
    "            \"text\": m.group()\n",
    "        })\n",
    "\n",
    "    for m in re.finditer(event_pattern, ground_truth, flags=re.IGNORECASE):\n",
    "        annotations.append({\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "            \"label\": \"EVENT\",\n",
    "            \"text\": m.group()\n",
    "        })\n",
    "\n",
    "    results[file_name] = annotations\n",
    "\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616fc0c-2462-4002-ad92-a39b0a202385",
   "metadata": {},
   "source": [
    "#### 2.2 Regeln + Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a914c0e0-4d87-43fb-a7c7-7dced53a0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "JSON_PATH = \"../../data/data_annotated.json\"\n",
    "OUTPUT_FILE = \"../../data/NER/flair/ner_regex_with_flair_results.json\"\n",
    "\n",
    "\n",
    "def extract_flair_entities(text):\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    predicted = []\n",
    "\n",
    "    for entity in sentence.get_spans(\"ner\"):\n",
    "        predicted.append({\n",
    "            \"start\": entity.start_position,\n",
    "            \"end\": entity.end_position,\n",
    "            \"label\": entity.get_label(\"ner\").value,\n",
    "            \"text\": entity.text\n",
    "        })\n",
    "    return predicted\n",
    "\n",
    "\n",
    "\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "relevant_label = {\"LOC\"}\n",
    "\n",
    "results = {} #dictionary\n",
    "\n",
    "for eintrag in data:\n",
    "    file_name = eintrag.get(\"file_name\")\n",
    "    if not file_name:\n",
    "        continue\n",
    "\n",
    "    ground_truth = eintrag.get(\"text\", \"\") \n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    predicted = extract_flair_entities(ground_truth)\n",
    "    pred_spans = [\n",
    "    {\n",
    "        \"start\": e[\"start\"],\n",
    "        \"end\": e[\"end\"],\n",
    "        \"label\": e[\"label\"],\n",
    "        \"text\": e[\"text\"]\n",
    "    }\n",
    "    for e in predicted if e[\"label\"] in relevant_label\n",
    "    ]\n",
    "\n",
    "    annotations.extend(pred_spans)\n",
    "\n",
    "\n",
    "    for m in re.finditer(time_pattern, ground_truth):\n",
    "        annotations.append({\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "            \"label\": \"TIME\",\n",
    "            \"text\": m.group()\n",
    "        })\n",
    "\n",
    "    for m in re.finditer(date_pattern, ground_truth):\n",
    "        annotations.append({\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "            \"label\": \"DATE\",\n",
    "            \"text\": m.group()\n",
    "        })\n",
    "\n",
    "    for m in re.finditer(event_pattern, ground_truth, flags=re.IGNORECASE):\n",
    "        annotations.append({\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "            \"label\": \"EVENT\",\n",
    "            \"text\": m.group()\n",
    "        })\n",
    "\n",
    "    results[file_name] = annotations\n",
    "\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0fbf2-2f5f-4703-b88c-4592bd97039e",
   "metadata": {},
   "source": [
    "#### ---> die Auswertung findet im notebook [04_flair_evaluation_optimierung](04_flair_evaluation_optimierung.ipynb) statt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a85866-fc85-4ace-9f3a-b74ece082884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
